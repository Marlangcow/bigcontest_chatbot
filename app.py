import os
from dotenv import load_dotenv

import numpy as np
import pandas as pd
import streamlit as st

from transformers import AutoTokenizer, AutoModel
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import RetrievalQA
from langchain.retrievers import EnsembleRetriever
from sentence_transformers import util
from sentence_transformers import SentenceTransformer
from google.cloud import dialogflow_v2 as dialogflow
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain.docstore.document import Document
from transformers import BitsAndBytesConfig, AutoModelForCausalLM
from langchain_core.runnables import RunnableLambda
from langchain.chains import LLMChain
import google.generativeai as genai
from typing import List, Dict
from langchain_community.embeddings import (
    SentenceTransformerEmbeddings,
    HuggingFaceEmbeddings,
)
from langchain_community.retrievers import BM25Retriever
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS

import faiss

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸŠê°ê·¤í†¡")
st.markdown(STYLES, unsafe_allow_html=True)

# Streamlit App UId
st.title("ğŸŠê°ê·¤í†¡, ì œì£¼ë„ ì—¬í–‰ ë©”ì´íŠ¸")
st.info("ì œì£¼ë„ ì—¬í–‰ ë©”ì´íŠ¸ ê°ê·¤í†¡ì´ ì œì£¼ë„ì˜ ë°©ë°©ê³¡ê³¡ì„ ì•Œë ¤ì¤„ê²ŒğŸŒ´")

# ì´ë¯¸ì§€ ë¡œë“œ ì„¤ì •
if "image_loaded" not in st.session_state:
    st.session_state.image_loaded = True
    st.session_state.image_html = """
    <div style="display: flex; justify-content: center;">
        <img src="https://img4.daumcdn.net/thumb/R658x0.q70/?fname=https://t1.daumcdn.net/news/202105/25/linkagelab/20210525013157546odxh.jpg" alt="centered image" width="50%">
    </div>
    """

# # ì´ë¯¸ì§€ í‘œì‹œ (ì„¸ì…˜ ìƒíƒœì—ì„œ í™•ì¸)
# if st.session_state.image_loaded:
#     st.markdown(st.session_state.image_html, unsafe_allow_html=True)
#     # ì´ë¯¸ì§€ê°€ í‘œì‹œëœ í›„ ë‹¤ì‹œ ìƒíƒœë¥¼ Falseë¡œ ë³€ê²½í•˜ì—¬ ì¤‘ë³µ í‘œì‹œ ë°©ì§€
#     st.session_state.image_loaded = False

st.write("")  # ì—¬ë°± ì¶”ê°€

# .env íŒŒì¼ ê²½ë¡œ ì§€ì •
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")


# CSV íŒŒì¼ ë¡œë“œ
@st.cache_data

# CSV íŒŒì¼ ë¡œë“œ
def load_data():
    csv_file_paths = [
        "./data/review_documents.csv",
        "./data/mct_documents.csv",
        "./data/trrsrt_documents.csv",
    ]
    dfs = []

    with st.spinner("ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”. ê³§ ë‚˜ì™€ìš”!"):  # ì‚¬ìš©ì ì •ì˜ ìŠ¤í”¼ë„ˆ ë©”ì‹œì§€
        dfs = [pd.read_csv(csv_file_path) for csv_file_path in csv_file_paths]

    return dfs


dfs = load_data()


# FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
faiss_index_path = "./modules/faiss_index.index"

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ
faiss_index = faiss.read_index(faiss_index_path)


# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
@st.cache_data
def load_model():
    return SentenceTransformer("jhgan/ko-sroberta-multitask")


model_embedding = load_model()


# Google Generative AI API ì„¤ì •
chat_model = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    api_key=google_api_key,
    temperature=0.3,
    top_p=0.85,
    frequency_penalty=0.3,
)

# ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ Memory ì„¤ì •
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ë©€í‹°í„´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (COT ë°©ì‹ ì ìš©)
prompt_template = PromptTemplate(
    input_variables=["input_text", "search_results", "chat_history"],
    template="""
    ### ì—­í• 
    ë‹¹ì‹ ì€ ì œì£¼ë„ ë§›ì§‘ê³¼ ê´€ê´‘ì§€ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°›ì„ ë•Œ ë…¼ë¦¬ì ìœ¼ë¡œ ìƒê°í•œ í›„ ë‹¨ê³„ë³„ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    ë³µì¡í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ì²œì²œíˆ ìƒê°í•˜ê³  ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ì •ê²¨ìš´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

    ### Chain of Thought ë°©ì‹ ì ìš©:
    1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    2. ì§ˆë¬¸ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    3. ê·¸ í›„ì— ì‚¬ìš©ìê°€ ì œê³µí•œ ì •ë³´ë‚˜ ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ì„± ìˆëŠ” ë§›ì§‘ê³¼ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
    4. ë‹¨ê³„ë¥¼ ë‚˜ëˆ„ì–´ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.

    ### ì§€ì‹œì‚¬í•­
    1. ê²€ìƒ‰í•  ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ì‚¬ìš©ìì—ê²Œ ë°˜ë¬¸í•˜ì„¸ìš”. ì´ëŠ” ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤. ë‹¨, ë‘ë²ˆ ì´ìƒ ë°˜ë¬¸í•˜ì§€ ë§ˆì„¸ìš”. ë§Œì•½ ì‚¬ìš©ìê°€ ìœ„ì¹˜ë¥¼ ëª¨ë¥¸ë‹¤ë©´ ì œì¼ í‰ì ì´ ì¢‹ì€ 3ê°œì˜ ì‹ë‹¹+ì¹´í˜ì™€ 3ê°œì˜ ê´€ê´‘ì§€ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
    2. ë‹µë³€ì„ í•˜ëŠ” ê²½ìš° ì–´ë–¤ ë¬¸ì„œë¥¼ ì¸ìš©í–ˆëŠ”ì§€ (í‚¤:ê°’) ì—ì„œ í‚¤ëŠ” ì œì™¸í•˜ê³  ê°’ë§Œ ë‹µë³€ ë’¤ì— ì–¸ê¸‰í•˜ì„¸ìš”.
      (mct_docs: ì‹ í•œì¹´ë“œ ê°€ë§¹ì  - ìš”ì‹ì—…, month_docs: ë¹„ì§“ì œì£¼ - ì›”ë³„ ì¡°íšŒìˆ˜, wkday_docs: ë¹„ì§“ì œì£¼ - ìš”ì¼ë³„ ì¡°íšŒìˆ˜, mop_docs: ê´€ê´‘ì§€ í‰ì ë¦¬ë·°, menu_docs: ì¹´ì¹´ì˜¤ë§µ ê°€ê²Œ ë©”ë‰´, visit_docs: ë¹„ì§“ì œì£¼ - ì—¬í–‰ì§€ ì •ë³´, kakaomap_reviews_docs: ì¹´ì¹´ì˜¤ë§µ ë¦¬ë·°)
    4. ì¶”ì²œ ì´ìœ ì™€ ê±°ë¦¬, ì†Œìš” ì‹œê°„, í•µì‹¬í‚¤ì›Œë“œ 3ê°œ, í‰ì ê³¼ ë¦¬ë·°ë“¤ë„ ë³´ì—¬ì£¼ì„¸ìš”. ë§Œì•½ ë¦¬ë·°ê°€ ì—†ëŠ” ê³³ì´ë¼ë©´ ("ì•„ì§ ì‘ì„±ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.") ë¼ê³  í•´ì£¼ì„¸ìš”.
    5. 4ë²ˆì˜ ì§€ì‹œì‚¬í•­ê³¼ í•¨ê»˜ íŒë§¤ ë©”ë‰´ 2ê°œ, ê°€ê²©ì„ í•¨ê»˜ ì•Œë ¤ì£¼ì„¸ìš”.
    6. ì£¼ì†Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ ê²€ìƒ‰ë˜ëŠ” ì¥ì†Œë¥¼ ì•„ë˜ ì˜ˆì‹œ ë§í¬ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
      - ë„¤ì´ë²„ ì§€ë„ í™•ì¸í•˜ê¸°: (https://map.naver.com/p/search/ì œì£¼ë„+<place>ì¥ì†Œëª…</place>)
    7. ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì‹ë‹¹ê³¼ ê´€ê´‘ì§€ëª…ì„ ì¶”ì²œí•´ì£¼ì–´ì•¼ í•˜ë©°, %%í‘ë¼ì§€ ë§›ì§‘, íšŸì§‘ 1 ë“± ê°€ê²Œëª…ì´ ëª…í™•í•˜ì§€ ì•Šì€ ë‹µë³€ì€ í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
    8. ë¬¸ì¥ì´ êµ¬ë¶„ë˜ë„ë¡ ë¬¸ë‹¨ì„ êµ¬ë¶„í•´ì£¼ì„¸ìš”.

    ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©:
    {search_results}

    ëŒ€í™” ê¸°ë¡:
    {chat_history}

    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {input_text}

    ë…¼ë¦¬ì ì¸ ì‚¬ê³  í›„ ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ë‹µë³€:
    """,
)


# ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def search_faiss(query_embedding, k=5):
    """
    FAISSì—ì„œ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì›ë³¸ ë°ì´í„° ë°˜í™˜
    """
    # FAISS ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰
    distances, indices = faiss_index.search(
        np.array(query_embedding, dtype=np.float32), k
    )

    # ê²€ìƒ‰ëœ ì¸ë±ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›ë³¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    search_results = []
    total_length = 0  # ì „ì²´ ê¸¸ì´ ì´ˆê¸°í™”

    for idx in indices[0]:
        found = False  # ì°¾ì€ ë°ì´í„°í”„ë ˆì„ ì²´í¬
        for df in dfs:
            if (
                total_length + len(df) > idx
            ):  # í˜„ì¬ ë°ì´í„°í”„ë ˆì„ì—ì„œ ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ì§€ ì²´í¬
                if idx - total_length >= 0 and idx - total_length < len(df):
                    search_results.append(df.iloc[idx - total_length])  # ì¸ë±ìŠ¤ ì¬ì¡°ì •
                found = True
                break
            total_length += len(df)  # ì „ì²´ ê¸¸ì´ì— ë°ì´í„°í”„ë ˆì„ ê¸¸ì´ ì¶”ê°€
        if found:  # ì´ë¯¸ ì°¾ì€ ê²½ìš° ë” ì´ìƒ ë°˜ë³µí•  í•„ìš” ì—†ìŒ
            continue

    return search_results


# ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(user_input):
    """
    ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ FAISS ê²€ìƒ‰ í›„ ì‘ë‹µ ìƒì„± (COT ì ìš©)
    """
    # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
    query_embedding = model_embedding.encode([user_input])

    # FAISS ê²€ìƒ‰ ìˆ˜í–‰
    search_results = search_faiss(query_embedding)

    # ê²€ìƒ‰ëœ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    search_results_str = "\n".join([result.to_string() for result in search_results])

    # PromptTemplateì— ê²€ìƒ‰ëœ ê²°ê³¼ì™€ ëŒ€í™” ê¸°ë¡ ì±„ìš°ê¸°
    filled_prompt = prompt_template.format(
        input_text=user_input,
        search_results=search_results_str,
        chat_history=memory.load_memory_variables({})["chat_history"],
    )

    # 1íšŒ í˜¸ì¶œì—ì„œ 5000 í† í° ì œí•œì´ë¯€ë¡œ ì ì ˆí•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    response_parts = []
    while filled_prompt:
        # ìµœëŒ€ 5000 í† í°ê¹Œì§€ ì˜ë¼ì„œ í˜¸ì¶œ
        part = filled_prompt[:5000]
        filled_prompt = filled_prompt[5000:]

        # Google Generative AI API í˜¸ì¶œ (ëŒ€ì‹  ì‚¬ìš©í•  ëª¨ë¸ë¡œ ìˆ˜ì • ê°€ëŠ¥)
        response = chat_model.invoke([{"role": "user", "content": part}])
        response_parts.append(response.content)

        # í˜¸ì¶œ íšŸìˆ˜ ì²´í¬
        if len(response_parts) >= 3:
            break  # ìµœëŒ€ 3íšŒ í˜¸ì¶œ ì œí•œ

    # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ ì €ì¥
    for part in response_parts:
        memory.save_context({"input": user_input}, {"output": part})

    # ìµœì¢… ì‘ë‹µ í•©ì¹˜ê¸°
    return "\n".join(response_parts)


# ìŠ¤íŠ¸ë¦¼ë¦¿ ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
if "messages" not in st.session_state:
    st.session_state.messages = []  # messages ì´ˆê¸°í™”

# ì´ë¯¸ì§€ í‘œì‹œ (ì„¸ì…˜ ìƒíƒœ ìœ ì§€)
st.markdown(st.session_state.image_html, unsafe_allow_html=True)

# ì´ì „ ëŒ€í™” ë©”ì‹œì§€ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ë©”ì‹œì§€ê°€ ì•„ë‹ ê²½ìš° ìƒˆ ì‘ë‹µ ìƒì„±
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = generate_response(prompt)
                placeholder = st.empty()
                full_response = ""  # ì‘ë‹µ ì´ˆê¸°í™”

                # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                if isinstance(response, str):
                    full_response = response
                else:
                    full_response = response.text

                # ì „ì²´ ì‘ë‹µ í‘œì‹œ
                placeholder.markdown(full_response)
        message = {"role": "assistant", "content": full_response}
        st.session_state.messages.append(message)
