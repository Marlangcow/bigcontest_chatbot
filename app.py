import os
from dotenv import load_dotenv

import numpy as np
import pandas as pd
import streamlit as st
import langchain.chat_models

from transformers import AutoTokenizer, AutoModel
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
# from langchain.chains import ConversationChain
from sentence_transformers import SentenceTransformer
# from langchain.chat_models import ChatGoogleGenerativeAI

import torch
import faiss

# .env íŒŒì¼ ê²½ë¡œ ì§€ì •
google_api_key = os.getenv("GOOGLE_API_KEY")


# CSV íŒŒì¼ê³¼ .npy íŒŒì¼ ê²½ë¡œ ì„¤ì •
csv_file_paths = [
    'C:\Users\user\Downloads\bigcontest_chatbot\data\review_documents.csv',
    'C:\Users\user\Downloads\bigcontest_chatbot\data\mct_documents.csvv',
    'C:\Users\user\Downloads\bigcontest_chatbot\data\trrsrt_documents.csv'
]
dfs = [pd.read_csv(csv_file_path) for csv_file_path in csv_file_paths]

# FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
faiss_index_path = '.module/faiss_index.index'

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ
faiss_index = faiss.read_index(faiss_index_path)

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì˜ˆ: 'jhgan/ko-sroberta-multitask')
model_embedding = SentenceTransformer('jhgan/ko-sroberta-multitask')


# Google Generative AI API ì„¤ì •
chat_model = GoogleGenerativeAI(model='gemini-1.5-flash',
                                    api_key='AIzaSyAnl8_XMJ-rJgZ4mGBqsUuq8A4jGESxPAo',
                                    temperature=0.2,  
                                    top_p=0.85,       
                                    frequency_penalty=0.1  
)

# ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ Memory ì„¤ì •
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ë©€í‹°í„´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (COT ë°©ì‹ ì ìš©)
prompt_template = PromptTemplate(
    input_variables=["input_text", "search_results", "chat_history"],
    template="""..."""  # ì›ë³¸ í…œí”Œë¦¿ ë‚´ìš© ê·¸ëŒ€ë¡œ ì‚¬ìš©
)

# ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def search_faiss(query_embedding, k=5):
    # FAISSì—ì„œ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì›ë³¸ ë°ì´í„° ë°˜í™˜
    distances, indices = faiss_index.search(np.array(query_embedding, dtype=np.float32), k)
    search_results = []
    total_length = 0

    for idx in indices[0]:
        found = False
        for df in dfs:
            if total_length + len(df) > idx:
                if idx - total_length >= 0 and idx - total_length < len(df):
                    search_results.append(df.iloc[idx - total_length])
                found = True
                break
            total_length += len(df)
        if found:
            continue

    return search_results

# ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(user_input):
    query_embedding = model_embedding.encode([user_input])
    search_results = search_faiss(query_embedding)
    search_results_str = "\n".join([result.to_string() for result in search_results])
    
    filled_prompt = prompt_template.format(
        input_text=user_input,
        search_results=search_results_str,
        chat_history=memory.load_memory_variables({})["chat_history"]
    )

    response_parts = []
    while filled_prompt:
        part = filled_prompt[:5000]
        filled_prompt = filled_prompt[5000:]

        response = chat_model.invoke([{"role": "user", "content": part}])
        response_parts.append(response.content)

        if len(response_parts) >= 3:
            break

    for part in response_parts:
        memory.save_context({"input": user_input}, {"output": part})

    return "\n".join(response_parts)

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="jeju-chatbot", page_icon="ğŸŒ´")
st.title("ğŸŒ´ ğŸŒ´ ì œì£¼ë„ ì—¬í–‰ ë©”ì´íŠ¸ AI")

# ì‚¬ìš©ì ì…ë ¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì…ë ¥ì°½
st.write("ì œì£¼ë„ íŠ¹ê¸‰ ê°€ì´ë“œ! ë§›ì§‘ë¶€í„° ì¹´í˜, ê´€ê´‘ì§€ê¹Œì§€ ì›í•˜ëŠ” ê³³ì„ ë§í•´ë´!")
message = st.text_input("ì°¾ê³  ìˆëŠ” ì¥ì†Œì˜ íŠ¹ì§•ì„ ì•Œë ¤ì¤˜.", key="input")

# ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
if 'history' not in st.session_state:
    st.session_state['history'] = []

if st.button("ì „ì†¡"):
    if message:
        try:
            # ëª¨ë¸ í˜¸ì¶œ ë° ì‘ë‹µ ë°›ê¸°
            response = generate_response(message)
            # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
            st.session_state['history'].append({"user": message, "bot": response})
        except Exception as e:
            st.error(f"ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

# ëŒ€í™” ê¸°ë¡ì„ í™”ë©´ì— ì¶œë ¥
if st.session_state['history']:
    for chat in st.session_state['history']:
        st.write(f"**ì‚¬ìš©ì**: {chat['user']}")
        st.write(f"**AI**: {chat['bot']}")


# import os
# import numpy as np
# import pandas as pd
# import streamlit as st

# from transformers import AutoTokenizer, AutoModel
# from langchain.memory import ConversationBufferMemory
# from langchain.prompts import PromptTemplate
# from langchain.chains import ConversationChain
# from sentence_transformers import SentenceTransformer
# import torch
# from tqdm import tqdm
# import faiss


# # ê²½ë¡œ ì„¤ì •
# data_path = '/.data'
# module_path = '.modules'

# # Gemini ì„¤ì •
# chat_model = ChatGoogleGenerativeAI(model='gemini-1.5-flash')

# # CSV íŒŒì¼ê³¼ .npy íŒŒì¼ ê²½ë¡œ ì„¤ì •
# csv_file_paths = [
#     './review_documents.csv',
#     './mct_documents.csv',
#     './trrsrt_documents.csv'
# ]
# dfs = [pd.read_csv(csv_file_path) for csv_file_path in csv_file_paths]

# # FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
# faiss_index_path = './faiss_index.index'

# # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
# faiss_index = faiss.read_index(faiss_index_path)

# from langchain.memory import ConversationBufferMemory
# from langchain.prompts import PromptTemplate
# from langchain.chains import ConversationChain
# from sentence_transformers import SentenceTransformer
# from langchain_google_genai import ChatGoogleGenerativeAI
# import numpy as np
# import faiss

# # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì˜ˆ: 'jhgan/ko-sroberta-multitask')
# model_embedding = SentenceTransformer('jhgan/ko-sroberta-multitask')

# # Google Generative AI API ì„¤ì •
# chat_model = ChatGoogleGenerativeAI(model='gemini-1.5-flash',
#                                     temperature=0.2,  # ë” ë‚®ì€ temperatureë¡œ ì„¤ì •í•´ í• ë£¨ì‹œë„¤ì´ì…˜ ì¤„ì„
#                                     top_p=0.85,        # top_pë¥¼ ì¡°ì •í•´ ë” ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë‹µë³€ ìƒì„±
#                                     frequency_penalty=0.1  # ê°™ì€ ë‹¨ì–´ì˜ ë°˜ë³µì„ ì¤„ì´ê¸° ìœ„í•´ íŒ¨ë„í‹° ì¶”ê°€
# )

# # 2. ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ Memory ì„¤ì •
# memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# # 3. ë©€í‹°í„´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (COT ë°©ì‹ ì ìš©)
# prompt_template = PromptTemplate(
#     input_variables=["input_text", "search_results", "chat_history"],
#     template="""
#    ### ì—­í• 
#     ë‹¹ì‹ ì€ ì œì£¼ë„ ë§›ì§‘ê³¼ ê´€ê´‘ì§€ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°›ì„ ë•Œ ë…¼ë¦¬ì ìœ¼ë¡œ ìƒê°í•œ í›„ ë‹¨ê³„ë³„ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. ë³µì¡í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ì²œì²œíˆ ìƒê°í•˜ê³  ì ì ˆí•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

#     ### Chain of Thought ë°©ì‹ ì ìš©:
#     1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
#     2. ë¨¼ì € ì§ˆë¬¸ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
#     3. ê·¸ í›„ì— ì‚¬ìš©ìê°€ ì œê³µí•œ ì •ë³´ë‚˜ ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ì„± ìˆëŠ” ë§›ì§‘ê³¼ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
#     4. ë‹¨ê³„ë¥¼ ë‚˜ëˆ„ì–´ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.

#     ### ë‹¨ê³„ì  ì‚¬ê³ :
#     1. ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„
#     2. ìœ„ì¹˜ ì •ë³´ í™•ì¸
#     3. ê´€ë ¨ ë°ì´í„° ê²€ìƒ‰
#     4. ì¶”ì²œ ë§›ì§‘ ë° ê´€ê´‘ì§€ ì œê³µ
#     5. ì¶”ê°€ ì§ˆë¬¸ì— ëŒ€í•œ ì¹œê·¼í•œ ëŒ€í™” ìœ ì§€

#     ### ì§€ì‹œì‚¬í•­
#     ë‹¹ì‹ ì€ ì‚¬ìš©ìë¡œë¶€í„° ì œì£¼ë„ì˜ ë§›ì§‘(ì‹ë‹¹, ì¹´í˜ ë“±)ê³¼ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
#     1. ì‚¬ìš©ìê°€ ì•Œê³ ì í•˜ëŠ” ë™ë„¤(ì‹œêµ°êµ¬)ë¥¼ ì•Œë ¤ì¤„ ë•Œ ê¹Œì§€ ì‚¬ìš©ìì—ê²Œ ë°˜ë¬¸í•˜ì„¸ìš”. ì´ëŠ” ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤. ë‹¨, ìœ„ì¹˜ë¥¼ ë‘ë²ˆ ì´ìƒ ë°˜ë¬¸í•˜ì§€ ë§ˆì„¸ìš”. ë§Œì•½ ì‚¬ìš©ìê°€ ìœ„ì¹˜ë¥¼ ëª¨ë¥¸ë‹¤ë©´ ì œì¼ í‰ì ì´ ì¢‹ì€ 3ê°œì˜ ì‹ë‹¹+ì¹´í˜ì™€ 3ê°œì˜ ê´€ê´‘ì§€ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
#     2. ì¹œê·¼í•˜ê³  ì¬ë¯¸ìˆìœ¼ë©´ì„œë„ ì •ê²¹ê²Œ ì•ˆë‚´í•˜ì„¸ìš”.
#     3. source_idëŠ” ë¬¸ì„œ ë²ˆí˜¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹µë³€ì„ í•˜ëŠ” ê²½ìš° ëª‡ ë²ˆ ë¬¸ì„œë¥¼ ì¸ìš©í–ˆëŠ”ì§€ ë‹µë³€ ë’¤ì— ì–¸ê¸‰í•˜ì„¸ìš”.
#     4. ì¶”ì²œ í•  ë•Œ, ì¶”ì²œ ì´ìœ ì™€ ì†Œìš”ë˜ëŠ” ê±°ë¦¬, í‰ì ê³¼ ë¦¬ë·°ë“¤ë„ ë³´ì—¬ì¤˜. ë§Œì•½ ë¦¬ë·°ê°€ ì—†ëŠ” ê³³ì´ë¼ë©´ ("ì‘ì„±ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.") ë¼ê³  í•´ì£¼ì„¸ìš”.
#     5. 4ë²ˆì˜ ì§€ì‹œì‚¬í•­ê³¼ í•¨ê»˜ íŒë§¤ ë©”ë‰´ 2ê°œ, ê°€ê²©ë„ ì•Œë ¤ì£¼ì„¸ìš”.
#     6. ë§Œì•½ ê´€ê´‘ì§€ì™€ ì‹ë‹¹ì´ êµ¬ê¸€ê²€ìƒ‰ì—ì„œ ë‚˜ì˜¤ëŠ” ê³³ì´ë©´ ì§€ë„(map)ë§í¬ë„ ê°™ì´ ì²¨ë¶€í•´ì¤˜. ì§€ë„ ë§í¬ê°€ ì—†ëŠ” ê³³ì€ ì§€ë„ ì—¬ë¶€ë¥¼ ë…¸ì¶œí•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
#     7. ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì‹ë‹¹ê³¼ ê´€ê´‘ì§€ëª…ì„ ì¶”ì²œí•´ì£¼ì–´ì•¼ í•˜ë©°, %%í‘ë¼ì§€ ë§›ì§‘, íšŸì§‘ 1 ë“± ê°€ê²Œëª…ì´ ëª…í™•í•˜ì§€ ì•Šì€ ë‹µë³€ì€ í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.

#     ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©:
#     {search_results}

#     ëŒ€í™” ê¸°ë¡:
#     {chat_history}

#     ì‚¬ìš©ìì˜ ì§ˆë¬¸: {input_text}

#     ë…¼ë¦¬ì ì¸ ì‚¬ê³  í›„ ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ë‹µë³€:
#     """
# )

# # 4. ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± í•¨ìˆ˜
# def search_faiss(query_embedding, k=5):
#     """
#     FAISSì—ì„œ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì›ë³¸ ë°ì´í„° ë°˜í™˜
#     """
#     # FAISS ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰
#     distances, indices = faiss_index.search(np.array(query_embedding, dtype=np.float32), k)

#     # ê²€ìƒ‰ëœ ì¸ë±ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›ë³¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
#     search_results = []
#     total_length = 0  # ì „ì²´ ê¸¸ì´ ì´ˆê¸°í™”

#     for idx in indices[0]:
#         found = False  # ì°¾ì€ ë°ì´í„°í”„ë ˆì„ ì²´í¬
#         for df in dfs:
#             if total_length + len(df) > idx:  # í˜„ì¬ ë°ì´í„°í”„ë ˆì„ì—ì„œ ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ì§€ ì²´í¬
#                 if idx - total_length >= 0 and idx - total_length < len(df):
#                     search_results.append(df.iloc[idx - total_length])  # ì¸ë±ìŠ¤ ì¬ì¡°ì •
#                 found = True
#                 break
#             total_length += len(df)  # ì „ì²´ ê¸¸ì´ì— ë°ì´í„°í”„ë ˆì„ ê¸¸ì´ ì¶”ê°€
#         if found:  # ì´ë¯¸ ì°¾ì€ ê²½ìš° ë” ì´ìƒ ë°˜ë³µí•  í•„ìš” ì—†ìŒ
#             continue

#     return search_results




# # 5. ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (COT ë°©ì‹)
# def generate_response(user_input):
#     """
#     ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ FAISS ê²€ìƒ‰ í›„ ì‘ë‹µ ìƒì„± (COT ì ìš©)
#     """
#     # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
#     query_embedding = model_embedding.encode([user_input])

#     # FAISS ê²€ìƒ‰ ìˆ˜í–‰
#     search_results = search_faiss(query_embedding)

#     # ê²€ìƒ‰ëœ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
#     search_results_str = "\n".join([result.to_string() for result in search_results])


#     # PromptTemplateì— ê²€ìƒ‰ëœ ê²°ê³¼ì™€ ëŒ€í™” ê¸°ë¡ ì±„ìš°ê¸°
#     filled_prompt = prompt_template.format(
#         input_text=user_input,
#         search_results=search_results_str,
#         chat_history=memory.load_memory_variables({})["chat_history"]
#     )

#     # 1íšŒ í˜¸ì¶œì—ì„œ 5000 í† í° ì œí•œì´ë¯€ë¡œ ì ì ˆí•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
#     response_parts = []
#     while filled_prompt:
#         # ìµœëŒ€ 5000 í† í°ê¹Œì§€ ì˜ë¼ì„œ í˜¸ì¶œ
#         part = filled_prompt[:5000]
#         filled_prompt = filled_prompt[5000:]

#         # Google Generative AI API í˜¸ì¶œ (ëŒ€ì‹  ì‚¬ìš©í•  ëª¨ë¸ë¡œ ìˆ˜ì • ê°€ëŠ¥)
#         response = chat_model.invoke([{"role": "user", "content": part}])
#         response_parts.append(response.content)

#         # í˜¸ì¶œ íšŸìˆ˜ ì²´í¬
#         if len(response_parts) >= 3:
#             break  # ìµœëŒ€ 3íšŒ í˜¸ì¶œ ì œí•œ

#     # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ ì €ì¥
#     for part in response_parts:
#         memory.save_context({"input": user_input}, {"output": part})

#     # ìµœì¢… ì‘ë‹µ í•©ì¹˜ê¸°
#     return "\n".join(response_parts)

# # 6. ì±—ë´‡ ëŒ€í™” ë£¨í”„
# def chat():
#     print("ì±—ë´‡ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. 'exit'ì„ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
#     while True:
#         user_input = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
#         if user_input.lower() == "exit":
#             break
#         try:
#             answer = generate_response(user_input)
#             print("ì±—ë´‡ ì‘ë‹µ:", answer)
#         except Exception as e:
#             print("ì˜¤ë¥˜ ë°œìƒ:", str(e))

# # ì±—ë´‡ ì‹¤í–‰
# chat()

# # Streamlit í˜ì´ì§€ ì„¤ì •
# st.set_page_config(page_title="ChatGPT", page_icon="ğŸŒ´")
# st.title("ğŸŒ´ ì œì£¼ë„ ì—¬í–‰ ë©”ì´íŠ¸ AI")

# # ì‚¬ìš©ì ì…ë ¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì…ë ¥ì°½
# st.write("ì œì£¼ë„ íŠ¹ê¸‰ ê°€ì´ë“œ! ë§›ì§‘ë¶€í„° ì¹´í˜, ê´€ê´‘ì§€ê¹Œì§€ ì›í•˜ëŠ” ê³³ì„ ë§í•´ë´!")
# message = st.text_input("ì°¾ê³  ìˆëŠ” ì¥ì†Œì˜ íŠ¹ì§•ì„ ì•Œë ¤ì¤˜.", key="input")

# # ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
# if 'history' not in st.session_state:
#     st.session_state['history'] = []

# if st.button("ì „ì†¡"):
#     if message:
#         # ëª¨ë¸ í˜¸ì¶œ ë° ì‘ë‹µ ë°›ê¸°
#         response = model.invoke(message)
        
#         # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
#         st.session_state['history'].append({"user": message, "bot": response.content})

# # ëŒ€í™” ê¸°ë¡ì„ í™”ë©´ì— ì¶œë ¥
# if st.session_state['history']:
#     for chat in st.session_state['history']:
#         st.write(f"**ì‚¬ìš©ì**: {chat['user']}")
#         st.write(f"**AI**: {chat['bot']}")