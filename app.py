import os
import numpy as np
import pandas as pd
import streamlit as st

from transformers import AutoTokenizer, AutoModel
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain.chains import ConversationChain
from sentence_transformers import SentenceTransformer
import torch
from tqdm import tqdm
import faiss

import streamlit as st

# ê²½ë¡œ ì„¤ì •
data_path = '/.data'
module_path = '.modules'

# Gemini ì„¤ì •
chat_model = ChatGoogleGenerativeAI(model='gemini-1.5-flash')

# CSV íŒŒì¼ê³¼ .npy íŒŒì¼ ê²½ë¡œ ì„¤ì •
csv_file_paths = [
    'FINAL_REVIEW.csv',
    'FINAL_MCT.csv',
    'FINAL_TRRSRT.csv'
]
npy_file_paths = ['V2/all_embeddings_v2.npy']
index_faiss_path = 'combined_db.index'
 
# CSV íŒŒì¼ ë¡œë“œ (low_memory=Falseë¡œ DtypeWarning ë°©ì§€)
dfs = [pd.read_csv(csv_file_path, low_memory=False) for csv_file_path in csv_file_paths]

# .npy íŒŒì¼ ë¡œë“œ
embeddings = np.load(npy_file_paths[0])

# ì„ë² ë”© ë²¡í„°ì˜ ì°¨ì› í™•ì¸
dimension = embeddings[0].shape[1] 

# FAISS ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°
faiss_db = faiss.read_index(index_faiss_path)

# FAISS ì¸ë±ìŠ¤ì— ì €ì¥ëœ ì´ ë²¡í„° ê°œìˆ˜ 
faiss_db.add(embeddings)


# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì˜ˆ: 'jhgan/ko-sroberta-multitask')
model_embedding = SentenceTransformer('jhgan/ko-sroberta-multitask')


# ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ Memory ì„¤ì •
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ë©€í‹°í„´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (COT ë°©ì‹ ì ìš©)
prompt_template = PromptTemplate(
    input_variables=["input_text", "search_results", "chat_history"],
    template="""
   ### ì—­í• 
    ë‹¹ì‹ ì€ ì œì£¼ë„ ë§›ì§‘ê³¼ ê´€ê´‘ì§€ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°›ì„ ë•Œ ë…¼ë¦¬ì ìœ¼ë¡œ ìƒê°í•œ í›„ ë‹¨ê³„ë³„ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. ë³µì¡í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ì²œì²œíˆ ìƒê°í•˜ê³  ì ì ˆí•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

    ### Chain of Thought ë°©ì‹ ì ìš©:
    1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    2. ë¨¼ì € ì§ˆë¬¸ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    3. ê·¸ í›„ì— ì‚¬ìš©ìê°€ ì œê³µí•œ ì •ë³´ë‚˜ ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ì„± ìˆëŠ” ë§›ì§‘ê³¼ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
    4. ë‹¨ê³„ë¥¼ ë‚˜ëˆ„ì–´ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.

    ### ë‹¨ê³„ì  ì‚¬ê³ :
    1. ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„
    2. ìœ„ì¹˜ ì •ë³´ í™•ì¸
    3. ê´€ë ¨ ë°ì´í„° ê²€ìƒ‰
    4. ì¶”ì²œ ë§›ì§‘ ë° ê´€ê´‘ì§€ ì œê³µ
    5. ì¶”ê°€ ì§ˆë¬¸ì— ëŒ€í•œ ì¹œê·¼í•œ ëŒ€í™” ìœ ì§€

    ### ì§€ì‹œì‚¬í•­
    ë‹¹ì‹ ì€ ì‚¬ìš©ìë¡œë¶€í„° ì œì£¼ë„ì˜ ë§›ì§‘(ì‹ë‹¹, ì¹´í˜ ë“±)ê³¼ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
    1. ì‚¬ìš©ìê°€ ì•Œê³ ì í•˜ëŠ” ë™ë„¤(ì‹œêµ°êµ¬)ë¥¼ ì•Œë ¤ì¤„ ë•Œ ê¹Œì§€ ì‚¬ìš©ìì—ê²Œ ë°˜ë¬¸í•˜ì„¸ìš”. ì´ëŠ” ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤. ë‹¨, ìœ„ì¹˜ë¥¼ ë‘ë²ˆ ì´ìƒ ë°˜ë¬¸í•˜ì§€ ë§ˆì„¸ìš”. ë§Œì•½ ì‚¬ìš©ìê°€ ìœ„ì¹˜ë¥¼ ëª¨ë¥¸ë‹¤ë©´ ì œì¼ í‰ì ì´ ì¢‹ì€ 3ê°œì˜ ì‹ë‹¹+ì¹´í˜ì™€ 3ê°œì˜ ê´€ê´‘ì§€ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
    2. ì¹œê·¼í•˜ê³  ì¬ë¯¸ìˆìœ¼ë©´ì„œë„ ì •ê²¹ê²Œ ì•ˆë‚´í•˜ì„¸ìš”.
    3. source_idëŠ” ë¬¸ì„œ ë²ˆí˜¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹µë³€ì„ í•˜ëŠ” ê²½ìš° ëª‡ ë²ˆ ë¬¸ì„œë¥¼ ì¸ìš©í–ˆëŠ”ì§€ ë‹µë³€ ë’¤ì— ì–¸ê¸‰í•˜ì„¸ìš”.
    4. ì¶”ì²œ í•  ë•Œ, ì¶”ì²œ ì´ìœ ì™€ ì†Œìš”ë˜ëŠ” ê±°ë¦¬, í‰ì ê³¼ ë¦¬ë·°ë“¤ë„ ë³´ì—¬ì¤˜. ë§Œì•½ ë¦¬ë·°ê°€ ì—†ëŠ” ê³³ì´ë¼ë©´ ("ì‘ì„±ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.") ë¼ê³  í•´ì£¼ì„¸ìš”.
    5. 4ë²ˆì˜ ì§€ì‹œì‚¬í•­ê³¼ í•¨ê»˜ íŒë§¤ ë©”ë‰´ 2ê°œ, ê°€ê²©ë„ ì•Œë ¤ì£¼ì„¸ìš”.
    6. ë§Œì•½ ê´€ê´‘ì§€ì™€ ì‹ë‹¹ì´ êµ¬ê¸€ê²€ìƒ‰ì—ì„œ ë‚˜ì˜¤ëŠ” ê³³ì´ë©´ ì§€ë„(map)ë§í¬ë„ ê°™ì´ ì²¨ë¶€í•´ì¤˜. ì§€ë„ ë§í¬ê°€ ì—†ëŠ” ê³³ì€ ì§€ë„ ì—¬ë¶€ë¥¼ ë…¸ì¶œí•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
    7. ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì‹ë‹¹ê³¼ ê´€ê´‘ì§€ëª…ì„ ì¶”ì²œí•´ì£¼ì–´ì•¼ í•˜ë©°, %%í‘ë¼ì§€ ë§›ì§‘, %%íšŸì§‘ ë“±ì˜ ë‹µë³€ì€ í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.

    ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©:
    {search_results}

    ëŒ€í™” ê¸°ë¡:
    {chat_history}

    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {input_text}

    ë…¼ë¦¬ì ì¸ ì‚¬ê³  í›„ ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ë‹µë³€:
    """
)

# 4. ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def search_faiss(query_embedding, k=5):
    """
    FAISSì—ì„œ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì›ë³¸ ë°ì´í„° ë°˜í™˜
    """
    # FAISS ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰
    distances, indices = faiss_db.search(np.array(query_embedding, dtype=np.float32), k)

    # ê²€ìƒ‰ëœ ì¸ë±ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›ë³¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    search_results = []
    for idx in indices[0]:
        for df in dfs:
            if idx < len(df):
                search_results.append(df.iloc[idx])
                break
            else:
                idx -= len(df)  # ì¸ë±ìŠ¤ê°€ ì´ˆê³¼ë˜ë©´ ë‹¤ìŒ ë°ì´í„°ì…‹ìœ¼ë¡œ ë„˜ì–´ê°

    return search_results

# 5. ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (COT ë°©ì‹)
def generate_response(user_input):
    """
    ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ FAISS ê²€ìƒ‰ í›„ ì‘ë‹µ ìƒì„± (COT ì ìš©)
    """
    # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
    query_embedding = model_embedding.encode([user_input])

    # FAISS ê²€ìƒ‰ ìˆ˜í–‰
    search_results = search_faiss(query_embedding)

    # ê²€ìƒ‰ëœ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    search_results_str = "\n".join([str(result) for result in search_results])

    # PromptTemplateì— ê²€ìƒ‰ëœ ê²°ê³¼ì™€ ëŒ€í™” ê¸°ë¡ ì±„ìš°ê¸°
    filled_prompt = prompt_template.format(
        input_text=user_input,
        search_results=search_results_str,
        chat_history=memory.load_memory_variables({})["chat_history"]
    )

    # 1íšŒ í˜¸ì¶œì—ì„œ 5000 í† í° ì œí•œì´ë¯€ë¡œ ì ì ˆí•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    response_parts = []
    while filled_prompt:
        # ìµœëŒ€ 5000 í† í°ê¹Œì§€ ì˜ë¼ì„œ í˜¸ì¶œ
        part = filled_prompt[:5000]
        filled_prompt = filled_prompt[5000:]

        # Google Generative AI API í˜¸ì¶œ (ëŒ€ì‹  ì‚¬ìš©í•  ëª¨ë¸ë¡œ ìˆ˜ì • ê°€ëŠ¥)
        response = chat_model.invoke([{"role": "user", "content": part}])
        response_parts.append(response.content)

        # í˜¸ì¶œ íšŸìˆ˜ ì²´í¬
        if len(response_parts) >= 3:
            break  # ìµœëŒ€ 3íšŒ í˜¸ì¶œ ì œí•œ

    # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ ì €ì¥
    for part in response_parts:
        memory.save_context({"input": user_input}, {"output": part})

    # ìµœì¢… ì‘ë‹µ í•©ì¹˜ê¸°
    return "\n".join(response_parts)

# ì±—ë´‡ ëŒ€í™” ë£¨í”„
def chat():
    print("ì±—ë´‡ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. 'exit'ì„ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    while True:
        user_input = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if user_input.lower() == "exit":
            break
        try:
            answer = generate_response(user_input)
            print("ì±—ë´‡ ì‘ë‹µ:", answer)
        except Exception as e:
            print("ì˜¤ë¥˜ ë°œìƒ:", str(e))

# ì±—ë´‡ ì‹¤í–‰
chat()

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ChatGPT", page_icon="ğŸŒ´")
st.title("ğŸŒ´ ë¹…ì½˜í…ŒìŠ¤íŠ¸ ChatGPT")

# ì‚¬ìš©ì ì…ë ¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì…ë ¥ì°½
st.write("ì œì£¼ë„ ë§›ì§‘ ë‹¤ ì•Œë ¤ë“œë¦¼")
message = st.text_input("ì–´ë–¤ ë§›ì§‘ì„ ê°€ê³  ì‹¶ì–´?", key="input")

# ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
if 'history' not in st.session_state:
    st.session_state['history'] = []

# ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œ Ollama ëª¨ë¸ í˜¸ì¶œ
if st.button("ì „ì†¡"):
    if message:
        # ëª¨ë¸ í˜¸ì¶œ ë° ì‘ë‹µ ë°›ê¸°
        response = model.invoke(message)
        
        # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        st.session_state['history'].append({"user": message, "bot": response.content})

# ëŒ€í™” ê¸°ë¡ì„ í™”ë©´ì— ì¶œë ¥
if st.session_state['history']:
    for chat in st.session_state['history']:
        st.write(f"**ì‚¬ìš©ì**: {chat['user']}")
        st.write(f"**AI**: {chat['bot']}")


## Chathpt 
import os
import numpy as np
import pandas as pd
import streamlit as st
from transformers import AutoTokenizer, AutoModel
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from sentence_transformers import SentenceTransformer
import faiss

# ê²½ë¡œ ì„¤ì •
data_path = '/.data'
module_path = '.modules'

# Gemini ì„¤ì •
chat_model = ChatGoogleGenerativeAI(model='gemini-1.5-flash')

# CSV íŒŒì¼ê³¼ .npy íŒŒì¼ ê²½ë¡œ ì„¤ì •
csv_file_paths = [
    'FINAL_REVIEW.csv',
    'FINAL_MCT.csv',
    'FINAL_TRRSRT.csv'
]
npy_file_paths = ['V2/all_embeddings_v2.npy']
index_faiss_path = 'combined_db.index'

# CSV íŒŒì¼ ë¡œë“œ (low_memory=Falseë¡œ DtypeWarning ë°©ì§€)
dfs = [pd.read_csv(csv_file_path, low_memory=False) for csv_file_path in csv_file_paths]

# .npy íŒŒì¼ ë¡œë“œ
embeddings = np.load(npy_file_paths[0])

# FAISS ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°
faiss_db = faiss.read_index(index_faiss_path)

# FAISS ì¸ë±ìŠ¤ì— ì €ì¥ëœ ì´ ë²¡í„° ê°œìˆ˜ 
faiss_db.add(embeddings)

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ (ì˜ˆ: 'jhgan/ko-sroberta-multitask')
model_embedding = SentenceTransformer('jhgan/ko-sroberta-multitask')

# ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ Memory ì„¤ì •
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ë©€í‹°í„´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (COT ë°©ì‹ ì ìš©)
prompt_template = PromptTemplate(
    input_variables=["input_text", "search_results", "chat_history"],
    template="""(í”„ë¡¬í”„íŠ¸ ë‚´ìš©ì€ ìœ„ì™€ ë™ì¼)"""
)

# 4. ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def search_faiss(query_embedding, k=5):
    """
    FAISSì—ì„œ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì›ë³¸ ë°ì´í„° ë°˜í™˜
    """
    # FAISS ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰
    distances, indices = faiss_db.search(np.array(query_embedding, dtype=np.float32), k)

    # ê²€ìƒ‰ëœ ì¸ë±ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›ë³¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    search_results = []
    for idx in indices[0]:
        for df in dfs:
            if idx < len(df):
                search_results.append(df.iloc[idx])
                break
            else:
                idx -= len(df)  # ì¸ë±ìŠ¤ê°€ ì´ˆê³¼ë˜ë©´ ë‹¤ìŒ ë°ì´í„°ì…‹ìœ¼ë¡œ ë„˜ì–´ê°

    return search_results

# 5. ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± í•¨ìˆ˜ (COT ë°©ì‹)
def generate_response(user_input):
    """
    ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ FAISS ê²€ìƒ‰ í›„ ì‘ë‹µ ìƒì„± (COT ì ìš©)
    """
    # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
    query_embedding = model_embedding.encode([user_input])

    # FAISS ê²€ìƒ‰ ìˆ˜í–‰
    search_results = search_faiss(query_embedding)

    # ê²€ìƒ‰ëœ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    search_results_str = "\n".join([str(result) for result in search_results])

    # PromptTemplateì— ê²€ìƒ‰ëœ ê²°ê³¼ì™€ ëŒ€í™” ê¸°ë¡ ì±„ìš°ê¸°
    filled_prompt = prompt_template.format(
        input_text=user_input,
        search_results=search_results_str,
        chat_history=memory.load_memory_variables({})["chat_history"]
    )

    # 1íšŒ í˜¸ì¶œì—ì„œ 5000 í† í° ì œí•œì´ë¯€ë¡œ ì ì ˆí•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    response_parts = []
    while filled_prompt:
        # ìµœëŒ€ 5000 í† í°ê¹Œì§€ ì˜ë¼ì„œ í˜¸ì¶œ
        part = filled_prompt[:5000]
        filled_prompt = filled_prompt[5000:]

        # Google Generative AI API í˜¸ì¶œ (ëŒ€ì‹  ì‚¬ìš©í•  ëª¨ë¸ë¡œ ìˆ˜ì • ê°€ëŠ¥)
        response = chat_model.invoke([{"role": "user", "content": part}])
        response_parts.append(response.content)

        # í˜¸ì¶œ íšŸìˆ˜ ì²´í¬
        if len(response_parts) >= 3:
            break  # ìµœëŒ€ 3íšŒ í˜¸ì¶œ ì œí•œ

    # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ ì €ì¥
    for part in response_parts:
        memory.save_context({"input": user_input}, {"output": part})

    # ìµœì¢… ì‘ë‹µ í•©ì¹˜ê¸°
    return "\n".join(response_parts)

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ChatGPT", page_icon="ğŸŒ´")
st.title("ğŸŒ´ ì œì£¼ë„ ë§›ì§‘ ë° ê´€ê´‘ì§€ ì¶”ì²œ ì±—ë´‡")

# ì‚¬ìš©ì ì…ë ¥ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì…ë ¥ì°½
st.write("ì œì£¼ë„ ë§›ì§‘ê³¼ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œë°›ì•„ë³´ì„¸ìš”!")
message = st.text_input("ì–´ë–¤ ë§›ì§‘ì´ë‚˜ ê´€ê´‘ì§€ë¥¼ ì°¾ê³  ì‹¶ìœ¼ì‹ ê°€ìš”?", key="input")

# ëŒ€í™” ê¸°ë¡ì„ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
if 'history' not in st.session_state:
    st.session_state['history'] = []

# ë²„íŠ¼ì„ ëˆŒë €ì„ ë•Œ ëª¨ë¸ í˜¸ì¶œ
if st.button("ì§ˆë¬¸í•˜ê¸°"):
    if message:
        # ëª¨ë¸ í˜¸ì¶œ ë° ì‘ë‹µ ë°›ê¸°
        response = generate_response(message)  # ëª¨ë¸ í˜¸ì¶œì„ generate_responseë¡œ ìˆ˜ì •
        
        # ëŒ€í™” ê¸°ë¡ì— ì¶”ê°€
        st.session_state['history'].append({"user": message, "bot": response})

# ëŒ€í™” ê¸°ë¡ì„ í™”ë©´ì— ì¶œë ¥
if st.session_state['history']:
    for chat in st.session_state['history']:
        st.write(f"**ì‚¬ìš©ì**: {chat['user']}")
        st.write(f"**ì±—ë´‡**: {chat['bot']}")
