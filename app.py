import os
from dotenv import load_dotenv

import numpy as np
import pandas as pd
import streamlit as st
import langchain.chat_models

from transformers import AutoTokenizer, AutoModel
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from sentence_transformers import SentenceTransformer
from langchain_google_genai import ChatGoogleGenerativeAI

import faiss

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸŠê°ê·¤í†¡")

# Streamlit App UI
st.title("ğŸŠê°ê·¤í†¡, ì œì£¼ë„ ì—¬í–‰ ë©”ì´íŠ¸")
st.info("ì œì£¼ë„ ì—¬í–‰ ë©”ì´íŠ¸ ê°ê·¤í†¡ì´ ì œì£¼ë„ì˜ ë°©ë°©ê³¡ê³¡ì„ ì•Œë ¤ì¤„ê²ŒğŸŒ´")

# ì´ë¯¸ì§€ ë¡œë“œ ì„¤ì •
if 'image_loaded' not in st.session_state:
    st.session_state.image_loaded = True
    st.session_state.image_html = """
    <div style="display: flex; justify-content: center;">
        <img src="https://img4.daumcdn.net/thumb/R658x0.q70/?fname=https://t1.daumcdn.net/news/202105/25/linkagelab/20210525013157546odxh.jpg" alt="centered image" width="50%">
    </div>
    """

# # ì´ë¯¸ì§€ í‘œì‹œ (ì„¸ì…˜ ìƒíƒœì—ì„œ í™•ì¸)
# if st.session_state.image_loaded:
#     st.markdown(st.session_state.image_html, unsafe_allow_html=True)
#     # ì´ë¯¸ì§€ê°€ í‘œì‹œëœ í›„ ë‹¤ì‹œ ìƒíƒœë¥¼ Falseë¡œ ë³€ê²½í•˜ì—¬ ì¤‘ë³µ í‘œì‹œ ë°©ì§€
#     st.session_state.image_loaded = False

st.write("")  # ì—¬ë°± ì¶”ê°€

# .env íŒŒì¼ ê²½ë¡œ ì§€ì •
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")

# CSV íŒŒì¼ ë¡œë“œ
@st.cache_data

# CSV íŒŒì¼ ë¡œë“œ
def load_data():
    csv_file_paths = [
        './data/review_documents.csv',
        './data/mct_documents.csv',
        './data/trrsrt_documents.csv'
    ]
    dfs = []
    
    with st.spinner("ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”. ê³§ ë‚˜ì™€ìš”!"):  # ì‚¬ìš©ì ì •ì˜ ìŠ¤í”¼ë„ˆ ë©”ì‹œì§€
        dfs = [pd.read_csv(csv_file_path) for csv_file_path in csv_file_paths]
    
    return dfs

dfs = load_data()



# FAISS ì¸ë±ìŠ¤ íŒŒì¼ ê²½ë¡œ
faiss_index_path = './modules/faiss_index.index'

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ
faiss_index = faiss.read_index(faiss_index_path)

# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
@st.cache_data
def load_model():
    return SentenceTransformer('jhgan/ko-sroberta-multitask')

model_embedding = load_model()


# Google Generative AI API ì„¤ì •
chat_model = ChatGoogleGenerativeAI(model='gemini-1.5-flash',
                                    api_key=google_api_key,
                                    temperature=0.3,  
                                    top_p=0.85,       
                                    frequency_penalty=0.3)

# ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ Memory ì„¤ì •
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ë©€í‹°í„´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (COT ë°©ì‹ ì ìš©)
prompt_template = PromptTemplate(
    input_variables=["input_text", "search_results", "chat_history"],
    template="""
   ### ì—­í• 
    ë‹¹ì‹ ì€ ì œì£¼ë„ ë§›ì§‘ê³¼ ê´€ê´‘ì§€ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°›ì„ ë•Œ ë…¼ë¦¬ì ìœ¼ë¡œ ìƒê°í•œ í›„ ë‹¨ê³„ë³„ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤. ë³µì¡í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ì²œì²œíˆ ìƒê°í•˜ê³  ì ì ˆí•œ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

    ### Chain of Thought ë°©ì‹ ì ìš©:
    1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    2. ë¨¼ì € ì§ˆë¬¸ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    3. ê·¸ í›„ì— ì‚¬ìš©ìê°€ ì œê³µí•œ ì •ë³´ë‚˜ ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ì„± ìˆëŠ” ë§›ì§‘ê³¼ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
    4. ë‹¨ê³„ë¥¼ ë‚˜ëˆ„ì–´ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.

    ### ë‹¨ê³„ì  ì‚¬ê³ :
    1. ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„
    2. ìœ„ì¹˜ ì •ë³´ í™•ì¸
    3. ê´€ë ¨ ë°ì´í„° ê²€ìƒ‰
    4. ì¶”ì²œ ë§›ì§‘ ë° ê´€ê´‘ì§€ ì œê³µ
    5. ì¶”ê°€ ì§ˆë¬¸ì— ëŒ€í•œ ì¹œê·¼í•œ ëŒ€í™” ìœ ì§€

    ### ì§€ì‹œì‚¬í•­
    ë‹¹ì‹ ì€ ì‚¬ìš©ìë¡œë¶€í„° ì œì£¼ë„ì˜ ë§›ì§‘(ì‹ë‹¹, ì¹´í˜ ë“±)ê³¼ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
    1. ê²€ìƒ‰í•  ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ì‚¬ìš©ìì—ê²Œ ë°˜ë¬¸í•˜ì„¸ìš”. ì´ëŠ” ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤. ë‹¨, ë‘ë²ˆ ì´ìƒ ë°˜ë¬¸í•˜ì§€ ë§ˆì„¸ìš”. ë§Œì•½ ì‚¬ìš©ìê°€ ìœ„ì¹˜ë¥¼ ëª¨ë¥¸ë‹¤ë©´ ì œì¼ í‰ì ì´ ì¢‹ì€ 3ê°œì˜ ì‹ë‹¹+ì¹´í˜ì™€ 3ê°œì˜ ê´€ê´‘ì§€ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
    2. ì¹œê·¼í•˜ê³  ì¬ë¯¸ìˆìœ¼ë©´ì„œë„ ì •ê²¹ê²Œ ì•ˆë‚´í•˜ì„¸ìš”.
    3. source_idëŠ” ë¬¸ì„œ ë²ˆí˜¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹µë³€ì„ í•˜ëŠ” ê²½ìš° ëª‡ ë²ˆ ë¬¸ì„œë¥¼ ì¸ìš©í–ˆëŠ”ì§€ ë‹µë³€ ë’¤ì— '(ë¬¸ì„œ ë²ˆí˜¸: source_id1)' í˜•ì‹ìœ¼ë¡œ ë³´ì—¬ì£¼ì„¸ìš”.
    4. ì¶”ì²œ í•  ë•Œ, ì¶”ì²œ ì´ìœ ì™€ ì†Œìš”ë˜ëŠ” ê±°ë¦¬, í‰ì ê³¼ ë¦¬ë·°ë“¤ë„ ë³´ì—¬ì¤˜. ë§Œì•½ ë¦¬ë·°ê°€ ì—†ëŠ” ê³³ì´ë¼ë©´ ("ì‘ì„±ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.") ë¼ê³  í•´ì£¼ì„¸ìš”.
    5. 4ë²ˆì˜ ì§€ì‹œì‚¬í•­ê³¼ í•¨ê»˜ íŒë§¤ ë©”ë‰´ 2ê°œ, ê°€ê²©ë„ ì•Œë ¤ì£¼ì„¸ìš”.
    6. ìœ„ë„ì™€ ê²½ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ ê²€ìƒ‰ë˜ëŠ” êµ¬ê¸€ ì§€ë„ ë§í¬ë¥¼ ì°¾ì•„ì„œ 'êµ¬ê¸€ ì§€ë„ ë§í¬'ë¡œ í´ë¦­í•  ìˆ˜ ìˆë„ë¡ í•´ì£¼ì„¸ìš”. ë‹¨, ì§€ë„ ë§í¬ê°€ ì—†ëŠ” ê³³ì€ ì§€ë„ ë§í¬ë¼ëŠ” ë¬¸êµ¬ë¥¼ ì•„ì˜ˆ ë…¸ì¶œí•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
    7. ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì‹ë‹¹ê³¼ ê´€ê´‘ì§€ëª…ì„ ì¶”ì²œí•´ì£¼ì–´ì•¼ í•˜ë©°, %%í‘ë¼ì§€ ë§›ì§‘, íšŸì§‘ 1 ë“± ê°€ê²Œëª…ì´ ëª…í™•í•˜ì§€ ì•Šì€ ë‹µë³€ì€ í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
    8. ë¬¸ì¥ì´ êµ¬ë¶„ë˜ë„ë¡ ë¬¸ë‹¨ì„ êµ¬ë¶„í•´ì£¼ì„¸ìš”.

    ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©:
    {search_results}

    ëŒ€í™” ê¸°ë¡:
    {chat_history}

    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {input_text}

    ë…¼ë¦¬ì ì¸ ì‚¬ê³  í›„ ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ë‹µë³€:
    """
)

# ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def search_faiss(query_embedding, k=5):
    """
    FAISSì—ì„œ ìœ ì‚¬í•œ ë²¡í„°ë¥¼ ê²€ìƒ‰í•˜ì—¬ ì›ë³¸ ë°ì´í„° ë°˜í™˜
    """
    # FAISS ì¸ë±ìŠ¤ì—ì„œ ìœ ì‚¬í•œ ë²¡í„° ê²€ìƒ‰
    distances, indices = faiss_index.search(np.array(query_embedding, dtype=np.float32), k)

    # ê²€ìƒ‰ëœ ì¸ë±ìŠ¤ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì›ë³¸ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
    search_results = []
    total_length = 0  # ì „ì²´ ê¸¸ì´ ì´ˆê¸°í™”

    for idx in indices[0]:
        found = False  # ì°¾ì€ ë°ì´í„°í”„ë ˆì„ ì²´í¬
        for df in dfs:
            if total_length + len(df) > idx:  # í˜„ì¬ ë°ì´í„°í”„ë ˆì„ì—ì„œ ìœ íš¨í•œ ì¸ë±ìŠ¤ì¸ì§€ ì²´í¬
                if idx - total_length >= 0 and idx - total_length < len(df):
                    search_results.append(df.iloc[idx - total_length])  # ì¸ë±ìŠ¤ ì¬ì¡°ì •
                found = True
                break
            total_length += len(df)  # ì „ì²´ ê¸¸ì´ì— ë°ì´í„°í”„ë ˆì„ ê¸¸ì´ ì¶”ê°€
        if found:  # ì´ë¯¸ ì°¾ì€ ê²½ìš° ë” ì´ìƒ ë°˜ë³µí•  í•„ìš” ì—†ìŒ
            continue

    return search_results


# ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(user_input):
    """
    ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ FAISS ê²€ìƒ‰ í›„ ì‘ë‹µ ìƒì„± (COT ì ìš©)
    """
    # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì„ë² ë”©ìœ¼ë¡œ ë³€í™˜
    query_embedding = model_embedding.encode([user_input])

    # FAISS ê²€ìƒ‰ ìˆ˜í–‰
    search_results = search_faiss(query_embedding)

    # ê²€ìƒ‰ëœ ê²°ê³¼ë¥¼ í…ìŠ¤íŠ¸ í˜•ì‹ìœ¼ë¡œ ë³€í™˜
    search_results_str = "\n".join([result.to_string() for result in search_results])

    # PromptTemplateì— ê²€ìƒ‰ëœ ê²°ê³¼ì™€ ëŒ€í™” ê¸°ë¡ ì±„ìš°ê¸°
    filled_prompt = prompt_template.format(
        input_text=user_input,
        search_results=search_results_str,
        chat_history=memory.load_memory_variables({})["chat_history"]
    )

    # 1íšŒ í˜¸ì¶œì—ì„œ 5000 í† í° ì œí•œì´ë¯€ë¡œ ì ì ˆí•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    response_parts = []
    while filled_prompt:
        # ìµœëŒ€ 5000 í† í°ê¹Œì§€ ì˜ë¼ì„œ í˜¸ì¶œ
        part = filled_prompt[:5000]
        filled_prompt = filled_prompt[5000:]

        # Google Generative AI API í˜¸ì¶œ (ëŒ€ì‹  ì‚¬ìš©í•  ëª¨ë¸ë¡œ ìˆ˜ì • ê°€ëŠ¥)
        response = chat_model.invoke([{"role": "user", "content": part}])
        response_parts.append(response.content)

        # í˜¸ì¶œ íšŸìˆ˜ ì²´í¬
        if len(response_parts) >= 3:
            break  # ìµœëŒ€ 3íšŒ í˜¸ì¶œ ì œí•œ

    # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ ì €ì¥
    for part in response_parts:
        memory.save_context({"input": user_input}, {"output": part})

    # ìµœì¢… ì‘ë‹µ í•©ì¹˜ê¸°
    return "\n".join(response_parts)


# ìŠ¤íŠ¸ë¦¼ë¦¿ ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
if 'messages' not in st.session_state:
    st.session_state.messages = []  # messages ì´ˆê¸°í™”

# ì´ë¯¸ì§€ í‘œì‹œ (ì„¸ì…˜ ìƒíƒœ ìœ ì§€)
st.markdown(st.session_state.image_html, unsafe_allow_html=True)

# ì´ì „ ëŒ€í™” ë©”ì‹œì§€ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ê°€ ì–´ì‹œìŠ¤í„´íŠ¸ì˜ ë©”ì‹œì§€ê°€ ì•„ë‹ ê²½ìš° ìƒˆ ì‘ë‹µ ìƒì„±
    if st.session_state.messages[-1]["role"] != "assistant":
        with st.chat_message("assistant"):
            with st.spinner("Thinking..."):
                response = generate_response(prompt)
                placeholder = st.empty()
                full_response = ''  # ì‘ë‹µ ì´ˆê¸°í™”

                # ì‘ë‹µì„ ë¬¸ìì—´ë¡œ ë³€í™˜
                if isinstance(response, str):
                    full_response = response
                else:
                    full_response = response.text  

                # ì „ì²´ ì‘ë‹µ í‘œì‹œ
                placeholder.markdown(full_response)
        message = {"role": "assistant", "content": full_response}
        st.session_state.messages.append(message)
