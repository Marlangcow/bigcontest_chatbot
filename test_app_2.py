import os
from dotenv import load_dotenv
import numpy as np
import pandas as pd
import streamlit as st
from sentence_transformers import SentenceTransformer
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
import faiss

# Streamlit í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸŠê°ê·¤í†¡")

# Streamlit App UI
st.title("ğŸŠê°ê·¤í†¡, ì œì£¼ë„ ì—¬í–‰ ë©”ì´íŠ¸")
st.info("ì œì£¼ë„ ì—¬í–‰ ë©”ì´íŠ¸ ê°ê·¤í†¡ì´ ì œì£¼ë„ì˜ ë°©ë°©ê³¡ê³¡ì„ ì•Œë ¤ì¤„ê²ŒğŸŒ´")

# ì´ë¯¸ì§€ ë¡œë“œ ì„¤ì •
if 'image_loaded' not in st.session_state:
    st.session_state.image_html = """
    <div style="display: flex; justify-content: center;">
        <img src="https://img4.daumcdn.net/thumb/R658x0.q70/?fname=https://t1.daumcdn.net/news/202105/25/linkagelab/20210525013157546odxh.jpg" alt="centered image" width="50%">
    </div>
    """
    st.session_state.image_loaded = True

st.write("")  # ì—¬ë°± ì¶”ê°€

# .env íŒŒì¼ ê²½ë¡œ ì§€ì •
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY")

# CSV íŒŒì¼ ë¡œë“œ
@st.cache_data

# CSV íŒŒì¼ ë¡œë“œ
def load_data():
    csv_file_paths = [
        './data/review_documents.csv',
        './data/mct_documents.csv',
        './data/trrsrt_documents.csv'
    ]
    dfs = []
    
    with st.spinner("ì ì‹œë§Œ ê¸°ë‹¤ë ¤ ì£¼ì„¸ìš”. ê³§ ë‚˜ì™€ìš”!"):  # ì‚¬ìš©ì ì •ì˜ ìŠ¤í”¼ë„ˆ ë©”ì‹œì§€
        dfs = [pd.read_csv(csv_file_path) for csv_file_path in csv_file_paths]
    
    return dfs

dfs = load_data()

# LLMì—ì„œ ì‚¬ìš©í•  í•¨ìˆ˜ tool ìƒì„±
def create_tool(name, description, required_params):
    return {
        "type": "function",
        "function": {
            "name": name,
            "description": description,
            "parameters": {
                "type": "object",
                "properties": {param: {"type": "string", "description": f"{param}ì— ëŒ€í•œ ì„¤ëª…"} for param in required_params},
                "required": required_params
            }
        }
    }

# ê°ê°ì˜ íˆ´ ìƒì„±
review_tool = create_tool("get_review_data", "ë§›ì§‘ê³¼ ê´€ê´‘ì§€ ë¦¬ë·° ì •ë³´ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.", ["ì¥ì†Œ"])
mct_tool = create_tool("get_mct_data", "MCT ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.", ["ê°€ê²Œëª…"])
trrsrt_tool = create_tool("get_trrsrt_data", "ê´€ê´‘ì§€ ë¬¸ì„œì˜ ì •ë³´ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤.", ["ê´€ê´‘ì§€ëª…"])

tools = [review_tool, mct_tool, trrsrt_tool]


# FAISS ì¸ë±ìŠ¤ ë° ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
faiss_index_path = './modules/faiss_index.index'
faiss_index = faiss.read_index(faiss_index_path)


# ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
def load_model():
    return SentenceTransformer('jhgan/ko-sroberta-multitask')

model_embedding = load_model()


# LLM ì„¤ì •
chat_model = ChatGoogleGenerativeAI(
    model='gemini-1.5-flash',
    api_key=google_api_key,
    temperature=0.3,
    top_p=0.85,
    frequency_penalty=0.3
)


# ë©€í‹°í„´ ëŒ€í™”ë¥¼ ìœ„í•œ Memory ì„¤ì •
memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

# ë©€í‹°í„´ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì„¤ì • (COT ë°©ì‹ ì ìš©)
prompt_template = PromptTemplate(
    input_variables=["input_text", "search_results", "chat_history"],
    template="""
   ### ì—­í• 
    ë‹¹ì‹ ì€ ì œì£¼ë„ ë§›ì§‘ê³¼ ê´€ê´‘ì§€ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°›ì„ ë•Œ ë…¼ë¦¬ì ìœ¼ë¡œ ìƒê°í•œ í›„ ë‹¨ê³„ë³„ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    ë³µì¡í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ì²œì²œíˆ ìƒê°í•˜ê³  ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

    ### Chain of Thought ë°©ì‹ ì ìš©:
    1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    2. ë¨¼ì € ì§ˆë¬¸ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    3. ê·¸ í›„ì— ì‚¬ìš©ìê°€ ì œê³µí•œ ì •ë³´ë‚˜ ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ì„± ìˆëŠ” ë§›ì§‘ê³¼ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
    4. ë‹¨ê³„ë¥¼ ë‚˜ëˆ„ì–´ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.

    ### ë‹¨ê³„ì  ì‚¬ê³ :
    1. ì‚¬ìš©ì ì§ˆë¬¸ ë¶„ì„
    2. ìœ„ì¹˜ ì •ë³´ í™•ì¸
    3. ê´€ë ¨ ë°ì´í„° ê²€ìƒ‰
    4. ì¶”ì²œ ë§›ì§‘ ë° ê´€ê´‘ì§€ ì œê³µ
    5. ì¶”ê°€ ì§ˆë¬¸ì— ëŒ€í•œ ì¹œê·¼í•œ ëŒ€í™” ìœ ì§€

    ### ì§€ì‹œì‚¬í•­
    ë‹¹ì‹ ì€ ì‚¬ìš©ìë¡œë¶€í„° ì œì£¼ë„ì˜ ë§›ì§‘(ì‹ë‹¹, ì¹´í˜ ë“±)ê³¼ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•˜ëŠ” ì±—ë´‡ì…ë‹ˆë‹¤.
    1. ê²€ìƒ‰í•  ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ì‚¬ìš©ìì—ê²Œ ë°˜ë¬¸í•˜ì„¸ìš”. ì´ëŠ” ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤. ë‹¨, ë‘ë²ˆ ì´ìƒ ë°˜ë¬¸í•˜ì§€ ë§ˆì„¸ìš”. ë§Œì•½ ì‚¬ìš©ìê°€ ìœ„ì¹˜ë¥¼ ëª¨ë¥¸ë‹¤ë©´ ì œì¼ í‰ì ì´ ì¢‹ì€ 3ê°œì˜ ì‹ë‹¹+ì¹´í˜ì™€ 3ê°œì˜ ê´€ê´‘ì§€ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
    2. ì¹œê·¼í•˜ê³  ì¬ë¯¸ìˆìœ¼ë©´ì„œë„ ì •ê²¹ê²Œ ì•ˆë‚´í•˜ì„¸ìš”.
    3. source_idëŠ” ë¬¸ì„œ ë²ˆí˜¸ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë‹µë³€ì„ í•˜ëŠ” ê²½ìš° ëª‡ ë²ˆ ë¬¸ì„œë¥¼ ì¸ìš©í–ˆëŠ”ì§€ ë‹µë³€ ë’¤ì— '(ë¬¸ì„œ ë²ˆí˜¸: source_id1)' í˜•ì‹ìœ¼ë¡œ ë³´ì—¬ì£¼ì„¸ìš”.
    4. ì¶”ì²œ í•  ë•Œ, ì¶”ì²œ ì´ìœ ì™€ ì†Œìš”ë˜ëŠ” ê±°ë¦¬, í‰ì ê³¼ ë¦¬ë·°ë“¤ë„ ë³´ì—¬ì¤˜. ë§Œì•½ ë¦¬ë·°ê°€ ì—†ëŠ” ê³³ì´ë¼ë©´ ("ì‘ì„±ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.") ë¼ê³  í•´ì£¼ì„¸ìš”.
    5. 4ë²ˆì˜ ì§€ì‹œì‚¬í•­ê³¼ í•¨ê»˜ íŒë§¤ ë©”ë‰´ 2ê°œ, ê°€ê²©ë„ ì•Œë ¤ì£¼ì„¸ìš”.
    6. ìœ„ë„ì™€ ê²½ë„ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ ê²€ìƒ‰ë˜ëŠ” ì¥ì†Œë¥¼ https://map.naver.com/p/search/ì œì£¼ë„ <placename>ì¥ì†Œì´ë¦„</placename>ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”. ë‹¨, ì§€ë„ ë§í¬ê°€ ì—†ëŠ” ê³³ì€ ì§€ë„ ë§í¬ë¼ëŠ” ë¬¸êµ¬ë¥¼ ì•„ì˜ˆ ë…¸ì¶œí•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
        ì˜ˆì‹œ ë§í¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. <placename> íƒœê·¸ëŠ” ì•Œê¸° ì‰½ê²Œ êµ¬ë¶„ í•´ ë†“ì€ ê°’ì´ë©°, ì ˆëŒ€ ë§í¬ë‚´ì— ì‚½ì…ë˜ì–´ì„œëŠ” ì•ˆë©ë‹ˆë‹¤.
        - https://map.naver.com/p/search/ì œì£¼ë„+ìš°ì§„í•´ì¥êµ­/
        - https://map.naver.com/p/search/ì œì£¼ë„+ì¹´í˜ë´„ë‚ /
        - https://map.naver.com/p/search/ì œì£¼ë„+ê³ íì˜ì •ì›/
    7. ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì‹ë‹¹ê³¼ ê´€ê´‘ì§€ëª…ì„ ì¶”ì²œí•´ì£¼ì–´ì•¼ í•˜ë©°, %%í‘ë¼ì§€ ë§›ì§‘, íšŸì§‘ 1 ë“± ê°€ê²Œëª…ì´ ëª…í™•í•˜ì§€ ì•Šì€ ë‹µë³€ì€ í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
    8. ë¬¸ì¥ì´ êµ¬ë¶„ë˜ë„ë¡ ë¬¸ë‹¨ì„ êµ¬ë¶„í•´ì£¼ì„¸ìš”.

    ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©:
    {search_results}

    ëŒ€í™” ê¸°ë¡:
    {chat_history}

    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {input_text}

    ë…¼ë¦¬ì ì¸ ì‚¬ê³  í›„ ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ë‹µë³€:
    """
)

# ê²€ìƒ‰ ë° ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def search_faiss(query_embedding, k=5):
    distances, indices = faiss_index.search(np.array(query_embedding, dtype=np.float32), k)
    search_results = []

    for idx in indices[0]:
        for df in dfs:
            if idx < len(df):
                search_results.append(df.iloc[idx])
                break
            idx -= len(df)

    return search_results


# ëŒ€í™”í˜• ì‘ë‹µ ìƒì„± í•¨ìˆ˜
def generate_response(user_input):
    query_embedding = model_embedding.encode([user_input])
    search_results = search_faiss(query_embedding)

    search_results_str = "\n".join([result.to_string() for result in search_results])
    
    filled_prompt = prompt_template(
        input_text=user_input,
        search_results=search_results_str,
        chat_history=memory.load_memory_variables({})["chat_history"]
    )

    response = chat_model.invoke([{"role": "user", "content": filled_prompt}], tools=tools)

    # ì‘ë‹µ ì²˜ë¦¬
    result = response.content
    if response["finish_reason"] == "function_call":
        function_data = response["tool_call"]["arguments"]
        if "ë¦¬ë·°" in function_data:
            result = get_review_data(**function_data)
        elif "ê°€ê²Œëª…" in function_data:
            result = get_mct_data(**function_data)
        elif "ê´€ê´‘ì§€ëª…" in function_data:
            result = get_trrsrt_data(**function_data)

    memory.save_context({"input": user_input}, {"output": result})
    return result

    # 1íšŒ í˜¸ì¶œì—ì„œ 5000 í† í° ì œí•œì´ë¯€ë¡œ ì ì ˆí•˜ê²Œ í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
    response_parts = []
    while filled_prompt:
        # ìµœëŒ€ 5000 í† í°ê¹Œì§€ ì˜ë¼ì„œ í˜¸ì¶œ
        part = filled_prompt[:5000]
        filled_prompt = filled_prompt[5000:]

        # Google Generative AI API í˜¸ì¶œ (ëŒ€ì‹  ì‚¬ìš©í•  ëª¨ë¸ë¡œ ìˆ˜ì • ê°€ëŠ¥)
        response = chat_model.invoke([{"role": "user", "content": part}])
        response_parts.append(response.content)

        # í˜¸ì¶œ íšŸìˆ˜ ì²´í¬
        if len(response_parts) >= 3:
            break  # ìµœëŒ€ 3íšŒ í˜¸ì¶œ ì œí•œ

    # ë©”ëª¨ë¦¬ì— ëŒ€í™” ê¸°ë¡ ì €ì¥
    for part in response_parts:
        memory.save_context({"input": user_input}, {"output": part})

    # ìµœì¢… ì‘ë‹µ í•©ì¹˜ê¸°
    return "\n".join(response_parts)


# ìŠ¤íŠ¸ë¦¼ë¦¿ ì±—ë´‡ ì¸í„°í˜ì´ìŠ¤
if 'messages' not in st.session_state:
    st.session_state.messages = []

# ì´ë¯¸ì§€ í‘œì‹œ (ì„¸ì…˜ ìƒíƒœ ìœ ì§€)
st.markdown(st.session_state.image_html, unsafe_allow_html=True)

# ì´ì „ ëŒ€í™” ë©”ì‹œì§€ í‘œì‹œ
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.write(msg["content"])

if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)
    response = generate_response(prompt)
    st.session_state.messages.append({"role": "assistant", "content": response})
    with st.chat_message("assistant"):
        st.write(response)

