import os
from dotenv import load_dotenv

import numpy as np
import pandas as pd
import streamlit as st

from transformers import AutoTokenizer, AutoModel
from langchain.memory import ConversationBufferMemory
from langchain.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain.chains import RetrievalQA
from langchain.retrievers import EnsembleRetriever
from sentence_transformers import util
from sentence_transformers import SentenceTransformer
from google.cloud import dialogflow_v2 as dialogflow
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_community.docstore.in_memory import InMemoryDocstore
from langchain.docstore.document import Document
from transformers import BitsAndBytesConfig, AutoModelForCausalLM
from langchain_core.runnables import RunnableLambda
from langchain.chains import LLMChain
import google.generativeai as genai
from typing import List, Dict
from langchain_community.embeddings import (
    SentenceTransformerEmbeddings,
    HuggingFaceEmbeddings,
)
from langchain_community.retrievers import BM25Retriever
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.vectorstores import FAISS


import faiss
import json
import torch


# Streamlit App UI
st.set_page_config(page_title="ğŸŠê°ê·¤í†¡")

st.title("ğŸŠê°ê·¤í†¡, ì œì£¼ë„ ì—¬í–‰ ë©”ì´íŠ¸")

st.write("")

st.info("ì œì£¼ë„ ì—¬í–‰ ë©”ì´íŠ¸ ê°ê·¤í†¡ì´ ì œì£¼ë„ì˜ ë°©ë°©ê³¡ê³¡ì„ ì•Œë ¤ì¤„ê²Œ ğŸï¸")

# ì´ë¯¸ì§€ ë¡œë“œ ì„¤ì •
if "image_loaded" not in st.session_state:
    st.session_state.image_loaded = True
    st.session_state.image_html = """
    <div style="display: flex; justify-content: center;">
        <img src="https://img4.daumcdn.net/thumb/R658x0.q70/?fname=https://t1.daumcdn.net/news/202105/25/linkagelab/20210525013157546odxh.jpg" alt="centered image" width="50%">
    </div>
    """

# í…ìŠ¤íŠ¸ í‘œì‹œ
st.write("")

st.write("")

# Replicate Credentials
with st.sidebar:
    st.title("ğŸŠê°ê·¤í†¡ì´ ë‹¤ ì°¾ì•„ì¤„ê²ŒğŸŠ")

    st.write("")

    st.subheader("ì›í•˜ëŠ” #í‚¤ì›Œë“œë¥¼ ê³¨ë¼ë´")

    # selectbox ë ˆì´ë¸” ê³µë°± ì œê±°
    st.markdown(
        """
        <style>
        .stSelectbox label {  /* This targets the label element for selectbox */
            display: none;  /* Hides the label element */
        }
        .stSelectbox div[role='combobox'] {
            margin-top: -20px; /* Adjusts the margin if needed */
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    keywords = st.sidebar.selectbox(
        "",
        [
            "ì°©í•œê°€ê²©ì—…ì†Œ",
            "ëŸ­ì…”ë¦¬íŠ¸ë˜ë¸”ì¸ì œì£¼",
            "ìš°ìˆ˜ê´€ê´‘ì‚¬ì—…ì²´",
            "ë¬´ì¥ì• ê´€ê´‘",
            "ì•ˆì „ì—¬í–‰ìŠ¤íƒ¬í”„",
            "í–¥í† ìŒì‹",
            "í•œì‹",
            "ì¹´í˜",
            "í•´ë¬¼ëšë°°ê¸°",
            "ëª¸êµ­",
            "í•´ì¥êµ­",
            "ìˆ˜ì œë²„ê±°",
            "í‘ë¼ì§€",
            "í•´ì‚°ë¬¼",
            "ì¼ì‹",
        ],
        key="keywords",
    )

    st.write("")

    st.subheader("ì–´ë–¤ ì¥ì†Œê°€ ê¶ê¸ˆí•´?")

    # ë ˆì´ë¸” ê³µë°± ì œê±°
    st.markdown(
        """
        <style>
        .stRadio > label {
            display: none;
        }
        .stRadio > div {
            margin-top: -20px;
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    locations = st.sidebar.selectbox(
        "",
        (
            "êµ¬ì¢Œ",
            "ëŒ€ì •",
            "ì„œê·€í¬",
            "ì•ˆë•",
            "ìš°ë„",
            "ì• ì›”",
            "ì¡°ì²œ",
            "ì œì£¼ì‹œë‚´",
            "ì¶”ì",
            "í•œë¦¼",
            "í•œê²½",
        ),
    )
    st.write("")

    st.subheader("í‰ì  ëª‡ì  ì´ìƒì„ ì›í•´?")
    score = st.slider("ë¦¬ë·° í‰ì ", min_value=3.0, max_value=5.0, value=4.5, step=0.05)

# ì´ë©”ì¼ ë§í¬
st.sidebar.caption(
    "ğŸ“¨ ê°ê·¤í†¡ ì œì‘ìì—ê²Œ ì—°ë½í•˜ê³  ì‹¶ë‹¤ë©´? [Send email](mailto:happily2bus@gmail.com)"
)


st.write("")


st.write("")

st.write("")

# Store LLM generated responses
if "messages" not in st.session_state.keys():
    st.session_state.messages = [
        {"role": "assistant", "content": "ì–´ë–¤ ê³³ì„ ì°¾ì•„ì¤„ê¹Œ?"}
    ]

# Display or clear chat messages
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.write(message["content"])


def clear_chat_history():
    st.session_state.messages = [
        {"role": "assistant", "content": "ì–´ë–¤ ê³³ì„ ì°¾ì•„ì¤„ê¹Œ?"}
    ]


st.sidebar.button("ëŒ€í™” ì´ˆê¸°í™”", on_click=clear_chat_history)


# RAG

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Device is {device}.")


# .env íŒŒì¼ ê²½ë¡œ ì§€ì •
load_dotenv()
google_api_key = os.getenv("GOOGLE_API_KEY_1")

# -------------------------
# Step 1: Data Loading
# -------------------------


# JSON íŒŒì¼ ë¡œë“œ
def load_json_files(file_paths):
    data = {}
    for key, path in file_paths.items():
        try:
            with open(path, "r", encoding="utf-8") as f:
                data[key] = json.load(f)
        except FileNotFoundError:
            print(f"File not found: {path}")
        except json.JSONDecodeError:
            print(f"Error decoding JSON from file: {path}")
    return data


file_paths = {
    "mct": "/Users/naeun/bigcontest_chatbot/data/mct.json",
    "month": "/Users/naeun/bigcontest_chatbot/data/month.json",
    "wkday": "/Users/naeun/bigcontest_chatbot/data/wkday.json",
    "mop_sentiment": "/Users/naeun/bigcontest_chatbot/data/merge_mop_sentiment.json",
    "menu": "/Users/naeun/bigcontest_chatbot/data/mct_menus.json",
    "visit_jeju": "/Users/naeun/bigcontest_chatbot/data/visit_jeju.json",
    "kakaomap_reviews": "/Users/naeun/bigcontest_chatbot/data/kakaomap_reviews.json",
}

data = load_json_files(file_paths)


# -------------------------
# Step 2: Embedding and FAISS Setup
# -------------------------


# FAISS ì¸ë±ìŠ¤ ë¡œë“œ ì‹œ ì˜¤ë¥˜ ì²˜ë¦¬ í•¨ìˆ˜ ê°œì„ 
def load_faiss_indexes(index_paths):
    indexes = {}
    for key, path in index_paths.items():
        try:
            if not os.path.exists(path):
                print(f"Warning: Index file not found: {path}")
                continue  # íŒŒì¼ì´ ì—†ìœ¼ë©´ ê±´ë„ˆë›°ê¸°
            indexes[key] = faiss.read_index(path)  # FAISS ì¸ë±ìŠ¤ ë¡œë“œ
        except faiss.FaissException as e:
            print(f"FAISS error loading index '{key}': {e}")
        except Exception as e:
            print(f"Unexpected error loading index '{key}': {e}")
    return indexes


# FAISS ì¸ë±ìŠ¤ ê²½ë¡œ ì„¤ì •
index_paths = {
    "mct": "/Users/naeun/bigcontest_chatbot/data/faiss_index/mct_index_pq.faiss",
    "month": "/Users/naeun/bigcontest_chatbot/data/faiss_index/month_index_pq.faiss",
    "wkday": "/Users/naeun/bigcontest_chatbot/data/faiss_index/wkday_index_pq.faiss",
    # "mop": "/Users/naeun/bigcontest_chatbot/data/faiss_index/mop_db.faiss",  # ì£¼ì„ ì²˜ë¦¬ëœ ê²½ë¡œ
    "menus": "/Users/naeun/bigcontest_chatbot/data/faiss_index/menu.faiss",
    "visit": "/Users/naeun/bigcontest_chatbot/data/faiss_index/visit_jeju.faiss",
    "kakaomap_reviews": "/Users/naeun/bigcontest_chatbot/data/faiss_index/kakaomap_reviews.faiss",
}

# FAISS ì¸ë±ìŠ¤ ë¡œë“œ
faiss_indexes = load_faiss_indexes(index_paths)


# ë¦¬ìŠ¤íŠ¸ í•­ëª©ì—ì„œ 'ê°€ê²Œëª…'ì„ ì‚¬ìš©í•˜ì—¬ Document ê°ì²´ ìƒì„±
mct_docs = [
    Document(page_content=item["ê°€ê²Œëª…"], metadata=item) for item in data["mct"]
]
month_docs = [
    Document(page_content=item["ê´€ê´‘ì§€ëª…"], metadata=item) for item in data["month"]
]
wkday_docs = [
    Document(page_content=item["ê´€ê´‘ì§€ëª…"], metadata=item) for item in data["wkday"]
]
mop_docs = [
    Document(page_content=item["ê´€ê´‘ì§€ëª…"], metadata=item)
    for item in data["mop_sentiment"]
]
menu_docs = [
    Document(page_content=item["ê°€ê²Œëª…"], metadata=item) for item in data["menu"]
]
visit_docs = [
    Document(page_content=item["ê´€ê´‘ì§€ëª…"], metadata=item)
    for item in data["visit_jeju"]
]
kakaomap_reviews_docs = [
    Document(page_content=item["ê´€ê´‘ì§€ëª…"], metadata=item)
    for item in data["kakaomap_reviews"]
]


# -------------------------
# Step 3: Initialize jhgan/ko-sroberta-multitask Model
# -------------------------

# ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "jhgan/ko-sroberta-multitask"
device = "cuda" if torch.cuda.is_available() else "cpu"
tokenizer = AutoTokenizer.from_pretrained(model_name)
embedding_model = AutoModel.from_pretrained(model_name).to(device)


# HuggingFaceEmbeddings ê°ì²´ ì´ˆê¸°í™”
embedding = HuggingFaceEmbeddings(model_name=model_name)


bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16,
)

# ê° ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ FAISS DBì— ë„£ê¸°
mct_db = FAISS.from_documents(documents=mct_docs, embedding=embedding)
month_db = FAISS.from_documents(documents=month_docs, embedding=embedding)
wkday_db = FAISS.from_documents(documents=wkday_docs, embedding=embedding)
mop_db = FAISS.from_documents(documents=mop_docs, embedding=embedding)
menus_db = FAISS.from_documents(documents=menu_docs, embedding=embedding)
visit_db = FAISS.from_documents(documents=visit_docs, embedding=embedding)
kakaomap_reviews_db = FAISS.from_documents(
    documents=kakaomap_reviews_docs, embedding=embedding
)

# ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ê²€ìƒ‰ê¸°ë¡œ ì‚¬ìš©í•˜ê¸° ìœ„í•´ retriever ë³€ìˆ˜ì— í• ë‹¹
mct_retriever = mct_db.as_retriever()
month_retriever = month_db.as_retriever()
wkday_retriever = wkday_db.as_retriever()
mop_retriever = mop_db.as_retriever()
mct_menus_retriever = menus_db.as_retriever()
visit_retriever = visit_db.as_retriever()
kakaomap_reviews_retriever = kakaomap_reviews_db.as_retriever()


def initialize_retriever(
    db, search_type="mmr", k=4, fetch_k=10, lambda_mult=0.6, score_threshold=0.8
):
    return db.as_retriever(
        search_type=search_type,
        search_kwargs={
            "k": k,
            "fetch_k": fetch_k,
            "lambda_mult": lambda_mult,
            "score_threshold": score_threshold,
        },
    )


# ë¦¬ìŠ¤íŠ¸ë¡œ DBì™€ ì´ë¦„ì„ ë¬¶ì–´ì„œ ì²˜ë¦¬
dbs = {
    "mct": mct_db,
    "month": month_db,
    "wkday": wkday_db,
    "mop": mop_db,
    "menus": menus_db,
    "visit": visit_db,
    "kakaomap_reviews": kakaomap_reviews_db,
}

# ê° DBì— ëŒ€í•´ ë¦¬íŠ¸ë¦¬ë²„ ì´ˆê¸°í™”
retrievers = {name: initialize_retriever(db) for name, db in dbs.items()}

# BM25 ê²€ìƒ‰ê¸° ìƒì„±
mct_bm25_retriever = BM25Retriever.from_texts([doc.page_content for doc in mct_docs])
month_bm25_retriever = BM25Retriever.from_texts(
    [doc.page_content for doc in month_docs]
)
wkday_bm25_retriever = BM25Retriever.from_texts(
    [doc.page_content for doc in wkday_docs]
)
mop_bm25_retriever = BM25Retriever.from_texts([doc.page_content for doc in mop_docs])
mct_menus_bm25_retriever = BM25Retriever.from_texts(
    [doc.page_content for doc in menu_docs]
)
visit_bm25_retriever = BM25Retriever.from_texts(
    [doc.page_content for doc in visit_docs]
)
kakaomap_reviews_bm25_retriever = BM25Retriever.from_texts(
    [doc.page_content for doc in kakaomap_reviews_docs]
)


def initialize_ensemble_retriever(retrievers, weights):
    return EnsembleRetriever(retrievers=retrievers, weights=weights)


# ê° DBì— ëŒ€í•´ ë¦¬íŠ¸ë¦¬ë²„ì™€ BM25 ë¦¬íŠ¸ë¦¬ë²„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¶ì€ ë”•ì…”ë„ˆë¦¬
ensemble_retrievers = {
    "mct": (mct_retriever, mct_bm25_retriever),
    "month": (month_retriever, month_bm25_retriever),
    "wkday": (wkday_retriever, wkday_bm25_retriever),
    "mop": (mop_retriever, mop_bm25_retriever),
    "mct_menus": (mct_menus_retriever, mct_menus_bm25_retriever),
    "visit": (visit_retriever, visit_bm25_retriever),
    "kakaomap_reviews": (kakaomap_reviews_retriever, kakaomap_reviews_bm25_retriever),
}

# Ensemble retrievers ì´ˆê¸°í™”ë¥¼ ëª…í™•í•˜ê²Œ
ensemble_retrievers = {
    name: initialize_ensemble_retriever(
        retrievers=[retrievers[name], globals()[f"{name}_bm25_retriever"]],
        weights=[0.6, 0.4],
    )
    for name in retrievers.keys()
}


def flexible_function_call_search(query):
    # ì…ë ¥ ì¿¼ë¦¬ì˜ ì„ë² ë”©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    input_embedding = embedding.embed_query(query)

    # ë¦¬íŠ¸ë¦¬ë²„ì™€ í•´ë‹¹ ì„¤ëª… ì •ì˜
    retrievers_info = {
        "mct": {
            "retriever": mct_retriever,
            "description": "ì‹ë‹¹ ì •ë³´ ë° ì—°ì´ìš© ë¹„ì¤‘ ë° ê¸ˆì•¡ ë¹„ì¤‘",
        },
        "month": {
            "retriever": month_retriever,
            "description": "ê´€ê´‘ì§€ ì›”ë³„ ì¡°íšŒìˆ˜",
        },
        "wkday": {
            "retriever": wkday_retriever,
            "description": "ì£¼ë³„ ì¼ë³„ ì¡°íšŒìˆ˜ ë° ì—°ë ¹ë³„ ì„±ë³„ë³„ ì„ í˜¸ë„",
        },
        "mop": {
            "retriever": mop_retriever,
            "description": "ê´€ê´‘ì§€ ì „ì²´ ê°ì„±ë¶„ì„ ë°ì´í„°",
        },
        "mct_menus": {
            "retriever": mct_menus_retriever,
            "description": "ì‹ë‹¹ëª… ë° ë©”ë‰´ ë° ê¸ˆì•¡",
        },
        "visit": {
            "retriever": visit_retriever,
            "description": "ê´€ê´‘ì§€ í•µì‹¬ í‚¤ì›Œë“œ ë° ì •ë³´",
        },
        "kakaomap_reviews": {
            "retriever": kakaomap_reviews_retriever,
            "description": "ë¦¬ë·° ë°ì´í„°",
        },
    }

    # ì…ë ¥ ì¿¼ë¦¬ì˜ ì„ë² ë”©ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
    input_embedding = embedding.embed_query(query)

    # ê° ë¦¬íŠ¸ë¦¬ë²„ì˜ ì„¤ëª…ì„ ê¸°ë°˜ìœ¼ë¡œ ì„ë² ë”©ì„ ìƒì„±í•©ë‹ˆë‹¤.
    retriever_embeddings = {
        key: embedding.embed_query(info["description"])
        for key, info in retrievers_info.items()
    }

    # ì…ë ¥ ì¿¼ë¦¬ì™€ ê° ë¦¬íŠ¸ë¦¬ë²„ ì„¤ëª… ê°„ì˜ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
    similarities = {
        key: util.cos_sim(input_embedding, torch.tensor(embed)).item()
        for key, embed in retriever_embeddings.items()
    }

    # ìœ ì‚¬ë„ê°€ ì¼ì • ì„ê³„ê°’ì„ ë„˜ëŠ” ë¦¬íŠ¸ë¦¬ë²„ë¥¼ ì„ íƒ
    selected_retrievers = [
        key for key, sim in similarities.items() if sim > 0.7  # ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •
    ]

    # ìœ ì‚¬ë„ ë†’ì€ ë¦¬íŠ¸ë¦¬ë²„ë§Œ ì‚¬ìš©
    if selected_retrievers:
        results = []
        for retriever_key in selected_retrievers:
            retriever = retrievers_info[retriever_key]["retriever"]
            result = retriever.get_relevant_documents(query)
            results.extend(result)
        return results
    else:
        return "ê´€ë ¨ëœ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."


module_path = os.path.dirname(os.path.abspath(__file__))
locations = {
    "êµ¬ì¢Œ": "êµ¬ì¢Œ",
    "ëŒ€ì •": "ëŒ€ì •",
    "ì•ˆë•": "ì•ˆë•",
    "ìš°ë„": "ìš°ë„",
    "ì• ì›”": "ì• ì›”",
    "ì¡°ì²œ": "ì¡°ì²œ",
    "ì œì£¼ì‹œë‚´": "ì œì£¼ì‹œë‚´",
    "ì¶”ì": "ì¶”ì",
    "í•œë¦¼": "í•œë¦¼",
    "í•œê²½": "í•œê²½",
}


def generate_response_with_faiss(
    question,
    df,
    embeddings,
    model,
    embed_text,
    keywords,
    local,
    index_path=os.path.join(module_path, "faiss_index.faiss"),
    max_count=10,
    k=3,
    print_prompt=True,
):
    filtered_df = df

    # FAISS ì¸ë±ìŠ¤ë¥¼ íŒŒì¼ì—ì„œ ë¡œë“œ
    faiss_indexes = load_faiss_indexes(index_paths)

    # ê²€ìƒ‰ ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±
    query_embedding = embed_text(question).reshape(1, -1)

    # ê°€ì¥ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ê²€ìƒ‰ (3ë°°ìˆ˜)
    distances, indices = index.search(query_embedding, k * 3)

    # FAISSë¡œ ê²€ìƒ‰ëœ ìƒìœ„ kê°œì˜ ë°ì´í„°í”„ë ˆì„ ì¶”ì¶œ
    filtered_df = filtered_df.iloc[indices[0, :]].copy().reset_index(drop=True)

    # ì›¹í˜ì´ì§€ì˜ ì‚¬ì´ë“œë°”ì—ì„œ ì„ íƒí•˜ëŠ” í‚¤ì›Œë“œ, ì§€ì—­, ë¦¬ë·° í‰ì  ì¡°ê±´ êµ¬í˜„

    # í•µì‹¬ í‚¤ì›Œë“œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê°€ê²Œë“¤ë§Œ í•„í„°ë§
    if keywords == "ì°©í•œê°€ê²©ì—…ì†Œ":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "ì°©í•œê°€ê²©ì—…ì†Œ" in x)
        ].reset_index(drop=True)
    elif keywords == "ëŸ­ì…”ë¦¬íŠ¸ë˜ë¸”ì¸ì œì£¼":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "ëŸ­ì…”ë¦¬íŠ¸ë˜ë¸”ì¸ì œì£¼" in x)
        ].reset_index(drop=True)
    elif keywords == "ìš°ìˆ˜ê´€ê´‘ì‚¬ì—…ì²´":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "ìš°ìˆ˜ê´€ê´‘ì‚¬ì—…ì²´" in x)
        ].reset_index(drop=True)
    elif keywords == "ë¬´ì¥ì• ê´€ê´‘":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "ë¬´ì¥ì• ê´€ê´‘" in x)
        ].reset_index(drop=True)
    elif keywords == "ì•ˆì „ì—¬í–‰ìŠ¤íƒ¬í”„":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "ì•ˆì „ì—¬í–‰ìŠ¤íƒ¬í”„" in x)
        ].reset_index(drop=True)
    elif keywords == "í–¥í† ìŒì‹":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "í–¥í† ìŒì‹" in x)
        ].reset_index(drop=True)
    elif keywords == "í•œì‹":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "í•œì‹" in x)
        ].reset_index(drop=True)
    elif keywords == "ì¹´í˜":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "ì¹´í˜" in x)
        ].reset_index(drop=True)
    elif keywords == "í•´ë¬¼ëšë°°ê¸°":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "í•´ë¬¼ëšë°°ê¸°" in x)
        ].reset_index(drop=True)
    elif keywords == "ëª¸êµ­":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "ëª¸êµ­" in x)
        ].reset_index(drop=True)
    elif keywords == "í•´ì¥êµ­":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "í•´ì¥êµ­" in x)
        ].reset_index(drop=True)
    elif keywords == "ìˆ˜ì œë²„ê±°":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "ìˆ˜ì œë²„ê±°" in x)
        ].reset_index(drop=True)
    elif keywords == "í‘ë¼ì§€":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "í‘ë¼ì§€" in x)
        ].reset_index(drop=True)
    elif keywords == "í•´ì‚°ë¬¼":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "í•´ì‚°ë¬¼" in x)
        ].reset_index(drop=True)
    elif keywords == "ì¼ì‹":
        filtered_df = filtered_df[
            filtered_df["í•µì‹¬í‚¤ì›Œë“œ"].apply(lambda x: "ì¼ì‹" in x)
        ].reset_index(drop=True)

    # ì„ íƒëœ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì²˜ë¦¬
    if filtered_df.empty:
        return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    # ì§€ì—­ ì˜µì…˜
    if local in locations:
        local = locations[local]
    else:
        local = "ê¸°íƒ€"  # 'ê¸°íƒ€' ì§€ì—­ìœ¼ë¡œ ì²˜ë¦¬

    # ì„ íƒëœ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì²˜ë¦¬
    if filtered_df.empty:
        return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    # í‰ì ì— ë§ëŠ” ê°€ê²Œ í•„í„°ë§
    filtered_df = filtered_df[filtered_df["í‰ì "] >= score].reset_index(drop=True)

    # ì„ íƒëœ ê²°ê³¼ê°€ ì—†ìœ¼ë©´ ì²˜ë¦¬
    if filtered_df.empty:
        return "ì§ˆë¬¸ê³¼ ì¼ì¹˜í•˜ëŠ” ê°€ê²Œê°€ ì—†ìŠµë‹ˆë‹¤."

    # ì°¸ê³ í•  ì •ë³´ì™€ í”„ë¡¬í”„íŠ¸ êµ¬ì„±
    reference_info = ""
    for idx, row in filtered_df.iterrows():
        reference_info += f"{row['text']}\n"

    # ì‘ë‹µì„ ë°›ì•„ì˜¤ê¸° ìœ„í•œ í”„ë¡¬í”„íŠ¸ ìƒì„±
    prompt = (
        f"ì§ˆë¬¸: {question} íŠ¹íˆ {local}ì„ ì„ í˜¸í•´\nì°¸ê³ í•  ì •ë³´:\n{reference_info}\nì‘ë‹µ:"
    )

    if print_prompt:
        print("-----------------------------" * 3)
        print(prompt)
        print("-----------------------------" * 3)

    # ì‘ë‹µ ìƒì„±
    response = model.generate_content(prompt)

    return response


# Google Generative AI API ì„¤ì •
llm = ChatGoogleGenerativeAI(
    model="gemini-1.5-flash",
    temperature=0.2,  # ë” ë‚®ì€ temperatureë¡œ ì„¤ì •í•´ í• ë£¨ì‹œë„¤ì´ì…˜ ì¤„ì„
    top_p=0.85,  # top_pë¥¼ ì¡°ì •í•´ ë” ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë‹µë³€ ìƒì„±
    frequency_penalty=0.1,  # ê°™ì€ ë‹¨ì–´ì˜ ë°˜ë³µì„ ì¤„ì´ê¸° ìœ„í•´ íŒ¨ë„í‹° ì¶”ê°€
)

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

prompt_template = PromptTemplate(
    input_variables=["input_text", "search_results", "chat_history"],
    template="""
    ### ì—­í• 
    ë‹¹ì‹ ì€ ì œì£¼ë„ ë§›ì§‘ê³¼ ê´€ê´‘ì§€ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì§ˆë¬¸ì„ ë°›ì„ ë•Œ ë…¼ë¦¬ì ìœ¼ë¡œ ìƒê°í•œ í›„ ë‹¨ê³„ë³„ë¡œ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.
    ë³µì¡í•œ ì§ˆë¬¸ì¼ìˆ˜ë¡ ì²œì²œíˆ ìƒê°í•˜ê³  ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¹œê·¼í•˜ê³  ì •ê²¨ìš´ ë‹µë³€ì„ ì œê³µí•©ë‹ˆë‹¤.

    ### Chain of Thought ë°©ì‹ ì ìš©:
    1. ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë‹¨ê³„ë³„ë¡œ ë¶„ì„í•©ë‹ˆë‹¤.
    2. ì§ˆë¬¸ì˜ ìœ„ì¹˜ ì •ë³´ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
    3. ê·¸ í›„ì— ì‚¬ìš©ìê°€ ì œê³µí•œ ì •ë³´ë‚˜ ê²€ìƒ‰ëœ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê´€ë ¨ì„± ìˆëŠ” ë§›ì§‘ê³¼ ê´€ê´‘ì§€ë¥¼ ì¶”ì²œí•©ë‹ˆë‹¤.
    4. ë‹¨ê³„ë¥¼ ë‚˜ëˆ„ì–´ ì •ë³´ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.

    ### ì§€ì‹œì‚¬í•­
    1. ê²€ìƒ‰í•  ë‚´ìš©ì´ ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ë©´ ì‚¬ìš©ìì—ê²Œ ë°˜ë¬¸í•˜ì„¸ìš”. ì´ëŠ” ê°€ì¥ ì¤‘ìš”í•©ë‹ˆë‹¤. ë‹¨, ë‘ë²ˆ ì´ìƒ ë°˜ë¬¸í•˜ì§€ ë§ˆì„¸ìš”. ë§Œì•½ ì‚¬ìš©ìê°€ ìœ„ì¹˜ë¥¼ ëª¨ë¥¸ë‹¤ë©´ ì œì¼ í‰ì ì´ ì¢‹ì€ 3ê°œì˜ ì‹ë‹¹+ì¹´í˜ì™€ 3ê°œì˜ ê´€ê´‘ì§€ë¥¼ ì•ˆë‚´í•´ì£¼ì„¸ìš”.
    2. ë‹µë³€ì„ í•˜ëŠ” ê²½ìš° ì–´ë–¤ ë¬¸ì„œë¥¼ ì¸ìš©í–ˆëŠ”ì§€ (í‚¤:ê°’) ì—ì„œ í‚¤ëŠ” ì œì™¸í•˜ê³  ê°’ë§Œ ë‹µë³€ ë’¤ì— ì–¸ê¸‰í•˜ì„¸ìš”.
      (mct_docs: ì‹ í•œì¹´ë“œ ê°€ë§¹ì  - ìš”ì‹ì—…, month_docs: ë¹„ì§“ì œì£¼ - ì›”ë³„ ì¡°íšŒìˆ˜, wkday_docs: ë¹„ì§“ì œì£¼ - ìš”ì¼ë³„ ì¡°íšŒìˆ˜, mop_docs: ê´€ê´‘ì§€ í‰ì ë¦¬ë·°, menu_docs: ì¹´ì¹´ì˜¤ë§µ ê°€ê²Œ ë©”ë‰´, visit_docs: ë¹„ì§“ì œì£¼ - ì—¬í–‰ì§€ ì •ë³´, kakaomap_reviews_docs: ì¹´ì¹´ì˜¤ë§µ ë¦¬ë·°)
    4. ì¶”ì²œ ì´ìœ ì™€ ê±°ë¦¬, ì†Œìš” ì‹œê°„, í•µì‹¬í‚¤ì›Œë“œ 3ê°œ, í‰ì ê³¼ ë¦¬ë·°ë“¤ë„ ë³´ì—¬ì£¼ì„¸ìš”. ë§Œì•½ ë¦¬ë·°ê°€ ì—†ëŠ” ê³³ì´ë¼ë©´ ("ì•„ì§ ì‘ì„±ëœ ë¦¬ë·°ê°€ ì—†ìŠµë‹ˆë‹¤.") ë¼ê³  í•´ì£¼ì„¸ìš”.
    5. 4ë²ˆì˜ ì§€ì‹œì‚¬í•­ê³¼ í•¨ê»˜ íŒë§¤ ë©”ë‰´ 2ê°œ, ê°€ê²©ì„ í•¨ê»˜ ì•Œë ¤ì£¼ì„¸ìš”.
    6. ì£¼ì†Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‹¤ì œ ê²€ìƒ‰ë˜ëŠ” ì¥ì†Œë¥¼ ì•„ë˜ ì˜ˆì‹œ ë§í¬ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•˜ì„¸ìš”.
      - ë„¤ì´ë²„ ì§€ë„ í™•ì¸í•˜ê¸°: (https://map.naver.com/p/search/ì œì£¼ë„+<place>ì¥ì†Œëª…</place>)
    7. ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ” ì‹ë‹¹ê³¼ ê´€ê´‘ì§€ëª…ì„ ì¶”ì²œí•´ì£¼ì–´ì•¼ í•˜ë©°, %%í‘ë¼ì§€ ë§›ì§‘, íšŸì§‘ 1 ë“± ê°€ê²Œëª…ì´ ëª…í™•í•˜ì§€ ì•Šì€ ë‹µë³€ì€ í•˜ì§€ ë§ì•„ì£¼ì„¸ìš”.
    8. ë‹µë³€ ë‚´ìš©ì— ë”°ë¼ í°íŠ¸ì‚¬ì´ì¦ˆ, ë¶ˆë ›, ìˆœì„œë¥¼ í™œìš©í•˜ê³  ë¬¸ë‹¨ì„ êµ¬ë¶„í•˜ì—¬ ê°€ë…ì„±ì´ ì¢‹ê²Œ í•´ì£¼ì„¸ìš”.

    ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©:
    {search_results}

    ëŒ€í™” ê¸°ë¡:
    {chat_history}

    ì‚¬ìš©ìì˜ ì§ˆë¬¸: {input_text}

    ë…¼ë¦¬ì ì¸ ì‚¬ê³  í›„ ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ë‹µë³€:
    """,
)

# ì²´ì¸ ìƒì„±
chain = LLMChain(
    prompt=prompt_template,
    llm=llm,
    output_parser=StrOutputParser(),
)


# ì±—ë´‡ ëŒ€í™” ë£¨í”„
def chat():
    print("ì±—ë´‡ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤. 'exit'ì„ ì…ë ¥í•˜ë©´ ì¢…ë£Œë©ë‹ˆë‹¤.")
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    while True:
        user_input = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
        if user_input.lower() == "exit":
            break

        search_results = flexible_function_call_search(user_input)

        # ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ LLMì— ì „ë‹¬
        search_results_str = "\n".join([doc.page_content for doc in search_results])

        # ëŒ€í™” ê¸°ë¡ ë¶ˆëŸ¬ì˜¤ê¸°
        chat_history = memory.load_memory_variables({})["chat_history"]

        input_data = {
            "input_text": user_input,
            "search_results": search_results_str,
            "chat_history": chat_history,
        }

        output = chain(input_data)
        output_text = output.get("text", str(output))

        print("\nì±—ë´‡ ì‘ë‹µ:", output_text)
        memory.save_context({"input": user_input}, {"output": output_text})


# User-provided prompt
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.write(prompt)

# Generate a new response if last message is not from assistant
if st.session_state.messages[-1]["role"] != "assistant":
    with st.chat_message("assistant"):
        with st.spinner("Thinking..."):
            response = generate_response_with_faiss(
                prompt, df, embeddings, model, embed_text, keywords, local
            )
            placeholder = st.empty()
            full_response = ""

            # ë§Œì•½ responseê°€ GenerateContentResponse ê°ì²´ë¼ë©´, ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤.
            if isinstance(response, str):
                full_response = response
            else:
                full_response = response.text

            placeholder.markdown(full_response)
    message = {"role": "assistant", "content": full_response}
    st.session_state.messages.append(message)
